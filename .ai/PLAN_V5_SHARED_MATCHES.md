# Plan Unifi√© ‚Äî LevelUp v5.0 : Architecture Shared Matches

> **Date** : 2026-02-14  
> **Branche source** : `sprint14/isolation-backend-frontend`  
> **Branche cible** : `v5/shared-matches-migration`  
> **Tag final** : `v5.0.0`  
> **Statut** : Plan consolid√© ‚Äî Migration architecture radicale vers base de donn√©es partag√©e  
> **Dur√©e estim√©e** : 14-18 jours ouvr√©s  

---

## üéØ Objectif Global

**Migrer vers une architecture radicale avec base de donn√©es partag√©e** pour √©liminer la duplication massive des donn√©es de matchs entre joueurs partageant des parties communes.

### Probl√©matique R√©solue

- ‚ùå **Avant** : Duplication de 75-100% des donn√©es (Madina97294/Chocoboflor = 95% matchs communs)
- ‚úÖ **Apr√®s** : 1 match stock√© 1 seule fois, accessible par tous les joueurs
- üìä **Gains** : -69% stockage, -72% appels API, -73% temps de sync

### Architecture Cible

```
data/warehouse/
‚îú‚îÄ‚îÄ metadata.duckdb              # R√©f√©rentiels (existant)
‚îî‚îÄ‚îÄ shared_matches.duckdb        # ‚≠ê NOUVEAU : Base unique pour TOUS les matchs
    ‚îú‚îÄ‚îÄ match_registry           # Registre central (1 ligne par match global)
    ‚îú‚îÄ‚îÄ match_participants       # TOUS les joueurs de TOUS les matchs
    ‚îú‚îÄ‚îÄ highlight_events         # TOUS les √©v√©nements film√©s
    ‚îú‚îÄ‚îÄ medals_earned            # M√©dailles de TOUS les joueurs
    ‚îî‚îÄ‚îÄ xuid_aliases             # Mapping global xuid‚Üígamertag

data/players/{gamertag}/
‚îî‚îÄ‚îÄ stats.duckdb                 # ‚≠ê SIMPLIFI√â : Uniquement enrichissements personnels
    ‚îú‚îÄ‚îÄ player_match_enrichment  # performance_score, session_id, is_with_friends
    ‚îú‚îÄ‚îÄ teammates_aggregate      # Agr√©gats depuis mon point de vue
    ‚îú‚îÄ‚îÄ antagonists              # Rivalit√©s depuis mon point de vue
    ‚îú‚îÄ‚îÄ media_files              # MES fichiers m√©dias
    ‚îî‚îÄ‚îÄ media_match_associations # MES associations m√©dia‚Üîmatch
```

---

## üöÄ CHECKLIST DE D√âMARRAGE OBLIGATOIRE

> **√Ä accomplir AVANT de lancer toute modification de code**

### 1. Pr√©paration Environnement

```bash
# 1. Cr√©er la branche depuis sprint14
git checkout sprint14/isolation-backend-frontend
git pull origin sprint14/isolation-backend-frontend
git checkout -b v5/shared-matches-migration

# 2. V√©rifier l'√©tat de base
python -m pytest -q --ignore=tests/integration

# 3. Cr√©er un backup COMPLET des DBs actuelles
python scripts/backup_all_players.py --output backups/pre-v5-migration-$(date +%Y%m%d)

# 4. Documenter l'√©tat de base
python scripts/diagnose_all_dbs.py > .ai/v5-baseline-state.txt
```

### 2. Backups Critiques (OBLIGATOIRE)

‚ö†Ô∏è **Cette migration est IRR√âVERSIBLE sans backup**

- [ ] Backup de `data/players/*/stats.duckdb` (TOUTES les DBs joueur)
- [ ] Backup de `data/warehouse/metadata.duckdb`
- [ ] Sauvegarde du sch√©ma SQL actuel (`scripts/export_current_schema.sql`)
- [ ] Commit de r√©f√©rence tagu√© `pre-v5-migration`

### 3. Environnement Python Valid√©

- **Python** : 3.12.10 (`.venv` √† la racine)
- **Commande** : `python -m ...` (jamais d'appel direct aux binaires)
- **Tests baseline** : `python -m pytest -q --ignore=tests/integration` ‚Üí **DOIT PASSER**

### 4. Documentation des Plans en Attente

Plans √† incorporer dans le sprint appropri√© :
- ‚úÖ **PLAN_OPTIMISATION_SYNC.md** ‚Üí Sprint 6 (optimisation API)
- ‚úÖ **PLAN_AMELIORATION_TESTS.md** ‚Üí Sprint 7 (couverture tests)

---

## üìã Table des Mati√®res

1. [R√®gles de D√©veloppement Strictes](#1-r√®gles-de-d√©veloppement-strictes)
2. [Strat√©gie de Migration](#2-strat√©gie-de-migration)
3. [Sprints D√©taill√©s](#3-sprints-d√©taill√©s) (S0-S8)
4. [Protocole de Revue par Sprint](#4-protocole-de-revue-par-sprint)
5. [Matrice de Risques](#5-matrice-de-risques)
6. [Crit√®res de Livraison Globaux](#6-crit√®res-de-livraison-globaux)
7. [Plan de Rollback](#7-plan-de-rollback)

---

## 1. R√®gles de D√©veloppement Strictes

### 1.1 Principes Architecturaux Obligatoires

#### Modularit√©

- **1 module = 1 responsabilit√© claire**
- Max 500 lignes par module (800 si justifi√© et document√©)
- Pas de couplage circulaire entre modules
- Interfaces claires et document√©es

#### Simplicit√©

- **KISS** : Toujours choisir la solution la plus simple qui fonctionne
- Pas d'abstraction pr√©matur√©e
- Code auto-document√© (noms explicites > commentaires)
- Pas de "clever code" : pr√©f√©rer la lisibilit√©

#### Logique

- **DRY** : Pas de duplication de logique m√©tier
- S√©paration des pr√©occupations (DB / Business / UI)
- Flux de donn√©es unidirectionnel et pr√©visible
- Gestion d'erreurs exhaustive et explicite

### 1.2 Qualit√© Code Non N√©gociable

#### Tests

- **Avant modifications** : `pytest` DOIT passer √† 100%
- **Apr√®s sprint** : `pytest` DOIT passer √† 100% + nouveaux tests
- Couverture minimale : 80% sur code m√©tier (`src/data/`, `src/analysis/`)
- Tests d'int√©gration pour chaque migration de donn√©es

#### Type Hints

- Type hints **OBLIGATOIRES** sur toutes les fonctions publiques
- Validation Pydantic v2 pour les mod√®les de donn√©es
- `mypy --strict` doit passer (sauf cas document√©s)

#### Documentation

- Docstrings **OBLIGATOIRES** en fran√ßais pour API publiques
- README √† jour pour chaque nouvelle feature
- Changelog d√©taill√© (format Conventional Commits)
- Diagrammes d'architecture √† jour (`.ai/architecture/`)

### 1.3 Git Workflow

#### Commits

- Format : `<type>(<scope>): <description>`
- Types : `feat`, `fix`, `refactor`, `test`, `docs`, `perf`, `chore`
- 1 commit = 1 changement logique atomique
- Message explicite (pas de "fix", "update", "wip")

#### Revue de Code

- **Auto-revue obligatoire** avant commit
- Checklist de validation par sprint (voir ¬ß4)
- Tests passent localement ET en CI
- Pas de `# TODO` ou `# FIXME` sans ticket associ√©

### 1.4 Suivi de Progression

#### Marquage des t√¢ches termin√©es

- **OBLIGATOIRE** : Chaque t√¢che termin√©e DOIT √™tre marqu√©e `[x]` dans ce plan
- Les tableaux de t√¢ches utilisent ‚úÖ en pr√©fixe pour les t√¢ches compl√©t√©es
- Les livrables et gates de livraison utilisent `[x]` au lieu de `[ ]`
- Mettre √† jour ce fichier **imm√©diatement** apr√®s chaque sprint termin√©
- Un sprint n'est consid√©r√© termin√© que quand TOUTES ses gates sont `[x]`

### 1.5 Migration de Donn√©es

#### S√©curit√©

- **TOUJOURS** cr√©er un backup avant modification de sch√©ma
- Scripts de migration **idempotents** (re-ex√©cutables)
- Validation des donn√©es **AVANT** et **APR√àS** migration
- Journalisation exhaustive des op√©rations

#### Compatibilit√©

- Migration **progressive** : 1 joueur √† la fois, validation, puis suivant
- VIEWs de compatibilit√© pour l'UI pendant la transition
- Rollback plan document√© et test√©
- Pas de suppression de donn√©es avant validation finale

---

## 2. Strat√©gie de Migration

### 2.1 Philosophie : Big Bang Contr√¥l√©

**Approche** : Migration compl√®te en une seule phase BUT avec validation incr√©mentale

#### Pourquoi Big Bang ?

‚úÖ **Avantages** :
- Pas de code de compatibilit√© hybride √† maintenir
- Simplification imm√©diate de l'architecture
- ROI imm√©diat (pas d'attente de phases multiples)

‚ö†Ô∏è **Risques Att√©nu√©s** :
- Validation joueur par joueur (Chocoboflor ‚Üí Madina ‚Üí JGtm ‚Üí xxdame)
- Backups √† chaque √©tape
- Tests de r√©gression complets
- Rollback plan document√©

### 2.2 Ordre des Op√©rations

```
Sprint 0  : Audit & Backups                          (1j)
Sprint 1  : Infrastructure shared_matches.duckdb     (2j)
Sprint 2  : Migration des donn√©es (4 joueurs)        (3j)  ‚úÖ TERMIN√â
Sprint 3  : Refactoring Sync Engine                  (3j)  ‚úÖ TERMIN√â
Sprint 4  : Refactoring DuckDBRepository             (2j)
Sprint 5  : Refactoring UI (VIEWs ‚Üí Queries natives) (3j)
Sprint 6  : Optimisation API (PLAN_OPTIMISATION)     (2j)
Sprint 7  : Tests & Couverture (PLAN_TESTS)          (2j)
Sprint 8  : Finalisation & Release v5.0              (2j)
```

**Total** : 18 jours ouvr√©s (peut descendre √† 14j avec parall√©lisation S3/S4)

### 2.3 Donn√©es Partag√©es vs Personnelles

#### ‚úÖ Stockage COMMUN (shared_matches.duckdb)

Toutes les donn√©es extraites de `MatchStats.Players[]` (API collective) :

| Table | Contenu | Source API |
|-------|---------|------------|
| `match_registry` | M√©tadonn√©es match (map, playlist, scores) | `MatchInfo` |
| `match_participants` | outcome, team_id, rank, score, K/D/A, accuracy | `Players[]` |
| `medals_earned` | M√©dailles de TOUS les joueurs | `Players[].Medals[]` |
| `highlight_events` | Tous les kills/deaths film√©s | Films API |
| `xuid_aliases` | Mapping global xuid‚Üígamertag | `Players[].Gamertag` |

#### ‚úÖ Stockage PERSONNEL (players/{gt}/stats.duckdb)

Uniquement ce qui **NE PEUT PAS** √™tre obtenu collectivement :

| Table | Contenu | Raison |
|-------|---------|--------|
| `player_match_enrichment` | performance_score, session_id, is_with_friends | Calcul√© depuis MON historique |
| `teammates_aggregate` | Stats co√©quipiers agr√©g√©es | Depuis MON point de vue |
| `antagonists` | Rivalit√©s | Depuis MON point de vue |
| `media_files` | MES fichiers m√©dias | MES captures/vid√©os |
| `media_match_associations` | MES associations | MES fichiers associ√©s |

---

## 2bis. Analyses de Contexte Pr√©liminaires (Sprints 3-8)

> **Objectif** : Acc√©l√©rer le d√©marrage de chaque sprint en documentant √† l'avance les fichiers concern√©s, les fonctions r√©utilisables, les d√©pendances et les points d'attention.

---

### Sprint 3 ‚Äî Contexte Pr√©liminaire : Refactoring Sync Engine

#### Fichiers Principaux Concern√©s

| Fichier | Taille | R√¥le | Modifications Pr√©vues |
|---------|--------|------|----------------------|
| `src/data/sync/engine.py` | 1249 lignes | Moteur de sync principal | Ajouter d√©tection shared, m√©thodes `_process_known_match()` et `_process_new_match()` |
| `src/data/sync/transformers.py` | 1469 lignes | Transformations JSON‚ÜíDuckDB | Cr√©er/adapter `extract_all_medals()` pour TOUS les joueurs |
| `src/data/sync/batch_insert.py` | ~300 lignes | Insertions batch DuckDB | Nouvelles fonctions d'insertion vers shared |
| `src/data/sync/models.py` | ~200 lignes | Mod√®les Pydantic | Possiblement ajouter `SharedMatchStatus` model |

#### Fonctions Existantes R√©utilisables

```python
# src/data/sync/engine.py
async def _process_single_match(self, client, match_id, options) -> dict
    # Ligne 654 - √Ä splitter en _process_known_match() et _process_new_match()
    # Actuellement s√©quentiel : stats ‚Üí skill ‚Üí events

# src/data/sync/transformers.py (ligne 1095)
def extract_participants(match_json: dict) -> list[MatchParticipantRow]
    # D√âJ√Ä FONCTIONNEL - Extrait TOUS les joueurs depuis Players[]
    # Parfait pour peupler shared.match_participants

def extract_xuids_from_match(match_json: dict) -> list[int]  # ligne 1044
    # Utilis√© pour les appels skill - √Ä conserver

def extract_medals(stats_json: dict, xuid: str) -> list[MedalEarnedRow]  # ligne 1243
    # ACTUEL : 1 seul joueur
    # √Ä CR√âER : extract_all_medals() pour TOUS les joueurs
```

#### Points d'Attention Critiques

**1. Parall√©lisation API (actuellement s√©quentiel)**

```python
# AVANT (ligne 685-691 engine.py)
if options.with_skill and xuids:
    skill_json = await client.get_skill_stats(match_id, xuids)

if options.with_highlight_events:
    highlight_events = await client.get_highlight_events(match_id)

# √Ä REMPLACER PAR (asyncio.gather)
tasks = []
if options.with_skill and xuids:
    tasks.append(client.get_skill_stats(match_id, xuids))
else:
    tasks.append(asyncio.sleep(0))
    
if options.with_highlight_events:
    tasks.append(client.get_highlight_events(match_id))
else:
    tasks.append(asyncio.sleep(0))

results = await asyncio.gather(*tasks, return_exceptions=True)
skill_json = results[0] if not isinstance(results[0], Exception) else None
highlight_events = results[1] if not isinstance(results[1], Exception) else []
```

**2. Gestion du Lock DB (_db_lock)**

- `async with self._db_lock:` ‚Äî Ligne 730  
- ‚ö†Ô∏è Faudra un second lock pour shared_matches ou partager le m√™me ?  
- **D√©cision** : Lock s√©par√© `_shared_db_lock` pour √©viter contention

**3. Connexion Shared**

```python
# Ajouter dans __init__ (ligne 250)
self._shared_db_path: Path | None = None
self._shared_connection: duckdb.DuckDBPyConnection | None = None
self._shared_db_lock = asyncio.Lock()

# Nouvelle m√©thode
def _get_shared_connection(self) -> duckdb.DuckDBPyConnection:
    if self._shared_connection is None:
        self._shared_connection = duckdb.connect(str(self._shared_db_path))
        self._shared_connection.execute("SET enable_object_cache = true")
    return self._shared_connection
```

#### D√©pendances Sprint 1 & 2

- ‚úÖ `shared_matches.duckdb` cr√©√©e (Sprint 1)
- ‚úÖ Schema valid√© (6 tables) (Sprint 1)
- ‚ö†Ô∏è Donn√©es migr√©es pour 4 joueurs (Sprint 2) ‚Äî **Blocker si non termin√©**

#### Estimation de Complexit√©

| T√¢che | Complexit√© | Risque | Temps estim√© |
|-------|-----------|--------|--------------|
| D√©tection match partag√© | Faible | Faible | 1h |
| `_process_known_match()` | Moyenne | Moyen | 3h |
| `_process_new_match()` | Moyenne | Moyen | 3h |
| `extract_all_medals()` | Faible | Faible | 2h |
| Insertions vers shared | Moyenne | Moyen | 4h |
| Tests unitaires | Moyenne | Faible | 3h |

**Total** : ~16h (sur 20-22h pr√©vues)

---

### Sprint 4 ‚Äî Contexte Pr√©liminaire : Refactoring DuckDBRepository

#### Fichiers Principaux Concern√©s

| Fichier | Taille | R√¥le | Modifications Pr√©vues |
|---------|--------|------|----------------------|
| `src/data/repositories/duckdb_repo.py` | 1114 lignes | Repository principal | Ajouter ATTACH shared, refactorer queries |
| `src/data/repositories/_match_queries.py` | ~400 lignes | Queries matchs | Adapter pour lire depuis `shared.*` |
| `src/data/repositories/_roster_loader.py` | ~250 lignes | Chargement rosters | Modifier pour lire `shared.match_participants` |
| `src/data/repositories/_antagonists_repo.py` | ~200 lignes | Chargement antagonistes | Possiblement adapter si d√©pend du roster |

#### Pattern ATTACH Existant (R√©utilisable)

```python
# D√âJ√Ä IMPL√âMENT√â pour metadata (ligne 122-144)
def _get_connection(self) -> duckdb.DuckDBPyConnection:
    if self._connection is None:
        self._connection = duckdb.connect(str(self._player_db_path), read_only=self._read_only)
        
        # ATTACH metadata (existant)
        if self._metadata_db_path.exists() and "meta" not in self._attached_dbs:
            try:
                self._connection.execute(
                    f"ATTACH '{self._metadata_db_path}' AS meta (READ_ONLY)"
                )
                self._attached_dbs.add("meta")
            except Exception as e:
                # Gestion erreur "already attached"
                pass
    return self._connection

# √Ä AJOUTER : ATTACH shared_matches (m√™me pattern)
if self._shared_db_path.exists() and "shared" not in self._attached_dbs:
    self._connection.execute(
        f"ATTACH '{self._shared_db_path}' AS shared (READ_ONLY)"
    )
    self._attached_dbs.add("shared")
```

#### Queries Critiques √† Adapter

**1. load_matches() ‚Äî Ligne ~200**

```sql
-- AVANT (v4) : Tout depuis match_stats local
SELECT 
    match_id, xuid, kills, deaths, assists, accuracy,
    outcome, team_id, rank, score AS personal_score,
    start_time, map_name, playlist_name, mode_category
FROM match_stats
WHERE xuid = ?
ORDER BY start_time DESC

-- APR√àS (v5) : JOIN shared + enrichment
SELECT 
    -- Donn√©es communes depuis shared.match_participants
    p.match_id, p.xuid, p.kills, p.deaths, p.assists,
    p.outcome, p.team_id, p.rank, p.score AS personal_score,
    p.damage_dealt, p.damage_taken, p.shots_fired, p.shots_hit,
    
    -- M√©tadonn√©es depuis shared.match_registry
    r.start_time, r.end_time, r.map_name, r.playlist_name, 
    r.mode_category, r.is_ranked,
    
    -- Enrichissement personnel depuis player DB
    e.performance_score, e.session_id, e.session_label, e.is_with_friends
    
FROM shared.match_participants p
INNER JOIN shared.match_registry r ON r.match_id = p.match_id
LEFT JOIN player_match_enrichment e ON e.match_id = p.match_id
WHERE p.xuid = ?
ORDER BY r.start_time DESC
```

**2. load_match_participants() ‚Äî Ligne ~300**

```sql
-- AVANT : match_participants local (seulement les joueurs d√©j√† track√©s)
SELECT * FROM match_participants WHERE match_id = ?

-- APR√àS : shared.match_participants (TOUS les joueurs du match)
SELECT 
    p.match_id, p.xuid, p.team_id, p.outcome, p.rank,
    p.score, p.kills, p.deaths, p.assists,
    COALESCE(a.gamertag, 'Unknown') as gamertag
FROM shared.match_participants p
LEFT JOIN shared.xuid_aliases a ON a.xuid = p.xuid
WHERE p.match_id = ?
ORDER BY p.rank ASC
```

**3. load_highlight_events()**

```sql
-- AVANT : highlight_events local
SELECT * FROM highlight_events WHERE match_id = ?

-- APR√àS : shared.highlight_events
SELECT * FROM shared.highlight_events WHERE match_id = ?
```

#### Points d'Attention Critiques

**1. Gestion des DB Absentes**

```python
# Cas o√π shared_matches.duckdb n'existe pas encore
# (transition progressive ou environnement de test)
if not self._shared_db_path.exists():
    logger.warning(f"shared_matches.duckdb absent : {self._shared_db_path}")
    # Fallback sur les queries v4 ? Ou erreur explicite ?
    # D√©cision : Erreur explicite (pas de fallback hybride)
```

**2. Performances ATTACH**

- DuckDB 1.4.4+ : 1 fichier = 1 connexion exclusive
- ATTACH en READ_ONLY √©conomise la RAM
- `SET enable_object_cache = true` d√©j√† utilis√© (ligne ~130)

**3. Migration des Tests**

Tous les tests `test_duckdb_repository.py` (101 tests) devront √™tre adapt√©s :
- Mocker `shared_matches.duckdb`
- Cr√©er fixtures avec donn√©es partag√©es
- Valider les JOINs

#### Mixins Impact√©s

| Mixin | Fichier | Impact | Action |
|-------|---------|--------|--------|
| `MatchQueriesMixin` | `_match_queries.py` | ‚≠ê Fort | Refactorer toutes les queries |
| `RosterLoaderMixin` | `_roster_loader.py` | ‚≠ê Fort | Lire depuis `shared.match_participants` |
| `MaterializedViewsMixin` | `_materialized_views.py` | Moyen | V√©rifier compatibilit√© |
| `AntagonistsMixin` | `_antagonists_repo.py` | Faible | Possiblement adapter |

#### Estimation de Complexit√©

| T√¢che | Complexit√© | Risque | Temps estim√© |
|-------|-----------|--------|--------------|
| ATTACH shared_matches | Faible | Faible | 1h |
| Adapter load_matches() | Moyenne | Moyen | 2h |
| Adapter load_participants() | Faible | Faible | 1h |
| Adapter load_events() | Faible | Faible | 30min |
| Adapter load_medals() | Faible | Faible | 1h |
| Tests repository | Moyenne | Moyen | 3h |
| Tests int√©gration UI | Forte | √âlev√© | 3h |

**Total** : ~11.5h (sur 13-15h pr√©vues)

---

### Sprint 5 ‚Äî Contexte Pr√©liminaire : Refactoring UI Big Bang

#### Pages UI Inventori√©es (24 fichiers)

| Page | Fichier | Taille | Utilise `repo.load_*` | Complexit√© |
|------|---------|--------|----------------------|-----------|
| Career | `career.py` | ~400 lignes | ‚úÖ `load_career_progression()` | Faible |
| Match History | `match_history.py` | ~600 lignes | ‚úÖ `load_matches()` | Moyenne |
| Match View | `match_view.py` + helpers | ~800 lignes | ‚úÖ `load_match_participants()` | Forte |
| Timeseries | `timeseries.py` | ~220 lignes | ‚úÖ `load_matches()` | Faible |
| Teammates | `teammates.py` + modules | ~1200 lignes | ‚úÖ `load_matches_with_teammate()` | Forte |
| Maps | `maps.py` | ~350 lignes | ‚úÖ `load_matches()` | Faible |
| Modes | `modes.py` | ~350 lignes | ‚úÖ `load_matches()` | Faible |
| Medals | `medals.py` | ~300 lignes | ‚úÖ `load_matches()`, `load_medals_*()` | Moyenne |
| Media Library | `media_library.py` | ~500 lignes | ‚úÖ `load_matches()` | Moyenne |
| Session Compare | `session_compare.py` | ~450 lignes | ‚úÖ `load_matches()` | Moyenne |
| Win/Loss | `win_loss.py` | ~200 lignes | ‚úÖ `load_matches()` | Faible |
| Objective Analysis | `objective_analysis.py` | ~400 lignes | ‚úÖ `load_matches()` | Moyenne |

**Total** : 12 pages principales + 10 modules helpers = **22 fichiers**

#### Pattern de Refactoring Type

**CAS 1 : Page simple (load_matches uniquement)**

```python
# AVANT (v4) - Pas de changement visible
def show_timeseries_page(repo: DuckDBRepository):
    df = repo.load_matches(limit=500)
    # ... graphiques avec df

# APR√àS (v5) - Aucun changement !
def show_timeseries_page(repo: DuckDBRepository):
    df = repo.load_matches(limit=500)  # Maintenant JOIN shared + enrichment en interne
    # ... graphiques avec df (m√™me structure)
```

**CAS 2 : Page avec roster (match_participants)**

```python
# AVANT (v4) - Roster partiel (seulement joueurs track√©s)
roster = repo.load_match_participants(match_id)
# roster.shape = (2, N) si seulement 2 joueurs track√©s sur 8

# APR√àS (v5) - Roster complet (TOUS les joueurs du match)
roster = repo.load_match_participants(match_id)
# roster.shape = (8, N) syst√©matiquement
# ‚ö†Ô∏è Adapter l'UI si elle supposait roster partiel
```

**CAS 3 : Page avec m√©dailles**

```python
# AVANT (v4) - M√©dailles seulement du joueur principal
medals = repo.load_medals_for_match(match_id)

# APR√àS (v5) - Besoin de filtrer par xuid explicitement
medals = repo.load_medals_for_match(match_id, xuid=repo.xuid)
# Ou charger TOUTES les m√©dailles du match
all_medals = repo.load_medals_for_match(match_id, xuid=None)  # Si impl√©ment√©
```

#### Points d'Attention Critiques

**1. Changements de Colonnes**

| Colonne v4 | Colonne v5 | Impact |
|-----------|------------|--------|
| `my_team_score` | `team_0_score` / `team_1_score` | ‚ö†Ô∏è Calcul √† adapter |
| `enemy_team_score` | `team_0_score` / `team_1_score` | ‚ö†Ô∏è Calcul √† adapter |
| `score` | `personal_score` | Renommage |
| - | `duration_seconds` | Nouvelle (depuis registry) |
| - | `player_count` | Nouvelle (depuis registry) |

**2. Rosters Complets (8 joueurs au lieu de 1-2)**

Pages impact√©es :
- `match_view.py` ‚Äî Affichage tableau joueurs
- `teammates.py` ‚Äî D√©tection co√©quipiers
- `objective_analysis.py` ‚Äî Analyse contributions

Action : V√©rifier que les boucles et filtres g√®rent bien 8+ joueurs.

**3. Performance `st.plotly_chart()`**

Rappel r√®gle : **JAMAIS** `use_container_width=True` (d√©pr√©ci√©).

```python
# ‚ùå INTERDIT
st.plotly_chart(fig, use_container_width=True)

# ‚úÖ CORRECT
st.plotly_chart(fig, width="stretch")
```

#### VIEWs de Compatibilit√© √† Supprimer (Sprint 5.11)

Si cr√©√©es pendant Sprint 2-4 pour transition :

```sql
-- Exemple : VIEW match_stats pointant vers shared
DROP VIEW IF EXISTS match_stats;
DROP VIEW IF EXISTS match_participants;
-- etc.
```

**Script** : `scripts/remove_compat_views.py`

#### Tests UI Existants

Fichiers de tests UI √† adapter :

| Test | Fichier | Assertions √† v√©rifier |
|------|---------|----------------------|
| Career page | `test_career_page.py` | Affichage progression |
| Match View | `test_app_phase2.py` | Roster complet (8 joueurs) |
| Timeseries | `test_new_timeseries_sections.py` | Graphiques cumulatifs |
| Teammates | `test_teammates_refonte.py` | D√©tection co√©quipiers |
| Filters | `test_cross_page_filter_persistence.py` | Filtres persistants |

**Nouveaux tests** : `tests/ui/test_all_pages_v5.py` (smoke tests complets)

#### Estimation de Complexit√©

| T√¢che | Nb fichiers | Temps estim√© |
|-------|-------------|--------------|
| Audit queries existantes | 22 | 1h |
| Refactoring pages simples (Career, Timeseries, Maps, Modes, Win/Loss) | 5 | 5√ó2h = 10h |
| Refactoring pages moyennes (Match History, Medals, Media, Objective) | 4 | 4√ó2h = 8h |
| Refactoring pages complexes (Match View, Teammates, Session) | 3 | 3√ó2.5h = 7.5h |
| Suppression VIEWs compat | 1 script | 1h |
| Tests UI automatis√©s | 1 fichier | 4h |

**Total** : ~31.5h ‚Üí **Optimisation possible** : Parall√©liser pages simples ‚Üí ~22h r√©aliste

---

### Sprint 6 ‚Äî Contexte Pr√©liminaire : Optimisation API

#### Optimisations Identifi√©es (PLAN_OPTIMISATION_SYNC.md)

**1. Parall√©lisation Appels API**

```python
# ACTUELLEMENT (engine.py ligne 685-691)
# S√©quentiel : skill ‚Üí events (2√ólatence r√©seau)
skill_json = await client.get_skill_stats(match_id, xuids)
highlight_events = await client.get_highlight_events(match_id)

# CIBLE (asyncio.gather)
tasks = [
    client.get_skill_stats(match_id, xuids) if options.with_skill else asyncio.sleep(0),
    client.get_highlight_events(match_id) if options.with_highlight_events else asyncio.sleep(0),
]
results = await asyncio.gather(*tasks, return_exceptions=True)

# GAIN : -50% latence sur les appels parall√©lisables
```

**2. D√©sactivation Performance Score pendant Sync**

```python
# ACTUELLEMENT (engine.py ~ligne 900)
# Performance score calcul√© PENDANT le sync (bloque l'insertion)
if _PERF_SCORE_AVAILABLE:
    perf_score = compute_relative_performance_score(...)
    # Requiert charger TOUT l'historique ‚Üí lent

# CIBLE (d√©sactiver pendant sync)
# Marquer performance_score = NULL pendant sync
# Post-sync : batch_compute_performance_scores()
```

**Nouvelle fonction √† cr√©er** :

```python
async def batch_compute_performance_scores(self) -> int:
    """Calcule performance_score pour tous les matchs o√π NULL.
    
    Ex√©cut√© POST-sync pour ne pas bloquer l'insertion.
    Utilise Polars pour calcul vectoris√© sur tout l'historique.
    
    Returns:
        Nombre de matchs mis √† jour.
    """
    # 1. Charger TOUS les matchs depuis shared + enrichment
    # 2. Grouper par session
    # 3. Calculer perf scores en batch (Polars)
    # 4. UPDATE player_match_enrichment
```

**3. Batching des Insertions DB**

```python
# ACTUELLEMENT (engine.py ~ligne 730)
# Commit APR√àS CHAQUE match
async with self._db_lock:
    self._insert_match_row(match_row)
    conn.commit()  # ‚Üê Chaque match = 1 commit

# CIBLE (commit tous les 10 matchs)
batch_buffer = []
for match in matches:
    row = transform_match(match)
    batch_buffer.append(row)
    
    if len(batch_buffer) >= 10:
        async with self._db_lock:
            for row in batch_buffer:
                self._insert_match_row(row)
            conn.commit()  # ‚Üê 1 commit pour 10 matchs
        batch_buffer.clear()
```

**4. Rate Limit Augment√©**

```python
# ACTUELLEMENT (src/data/sync/api_client.py)
DEFAULT_RATE_LIMIT = 5  # req/s
parallel_matches = 3    # Matchs en parall√®le

# CIBLE (selon PLAN_OPTIMISATION)
DEFAULT_RATE_LIMIT = 10  # req/s
parallel_matches = 5     # Matchs en parall√®le

# V√©rifier limites API Halo :
# - Pas de limite document√©e stricte
# - Tests empiriques OK jusqu'√† 10 req/s
```

#### Fichiers √† Modifier

| Fichier | Modifications | Risque |
|---------|--------------|--------|
| `src/data/sync/engine.py` | Parall√©lisation API, batching commits, perf score d√©sactiv√© | Moyen |
| `src/data/sync/api_client.py` | Rate limit augment√© | Faible |
| `src/data/sync/models.py` | Nouveau champ `defer_performance_score` dans SyncOptions | Faible |
| `src/analysis/performance_score.py` | Adapter pour calcul batch (Polars) | Faible |

#### Gains Attendus (Calcul√©s)

| M√©trique | v4 (avant Sprint 6) | v5 Sprint 6 | Gain |
|----------|---------------------|-------------|------|
| Temps/match (nouveau) | 3s | 2s | **-33%** |
| Temps/match (partag√© 95%) | 1s | 0.5s | **-50%** |
| Sync 100 matchs nouveaux | 5 min | 3.5 min | **-30%** |
| Commits DB/100 matchs | 100 | 10 | **-90%** (I/O disque) |

#### Tests de Validation

```python
# tests/performance/test_sync_v5.py
@pytest.mark.benchmark
async def test_sync_100_matches_new():
    """Benchmark sync 100 matchs nouveaux."""
    start = time.time()
    result = await engine.sync_full(SyncOptions(max_matches=100))
    duration = time.time() - start
    
    assert duration < 180  # < 3 minutes
    assert result.matches_inserted == 100

@pytest.mark.benchmark
async def test_sync_100_matches_shared():
    """Benchmark re-sync 100 matchs partag√©s (d√©tection √©conomie API)."""
    # Pr√©-remplir shared_matches avec 100 matchs
    # Re-sync pour le m√™me joueur
    start = time.time()
    result = await engine.sync_full(SyncOptions(max_matches=100))
    duration = time.time() - start
    
    assert duration < 60  # < 1 minute (10√ó plus rapide)
    assert result.api_calls_saved >= 150  # ~1.5 appels √©conomis√©s par match
```

#### Estimation de Complexit√©

| T√¢che | Complexit√© | Temps estim√© |
|-------|-----------|--------------|
| Parall√©lisation API (asyncio.gather) | Faible | 2h |
| D√©sactiver perf score pendant sync | Faible | 1h |
| Cr√©er batch_compute_performance_scores() | Moyenne | 3h |
| Batching commits DB | Moyenne | 2h |
| Rate limit augment√© | Faible | 30min |
| Tests benchmark | Moyenne | 2h |
| Documentation optimisations | Faible | 1h |

**Total** : ~11.5h (sur 11-13h pr√©vues)

---

### Sprint 7 ‚Äî Contexte Pr√©liminaire : Tests & Couverture

#### √âtat Actuel de la Couverture (Estim√© baseline)

| Module | Fichiers | Couverture actuelle | Objectif v5 |
|--------|----------|---------------------|-------------|
| `src/data/sync/` | 8 fichiers | ~65% | **85%** |
| `src/data/repositories/` | 6 fichiers | ~70% | **85%** |
| `src/ui/pages/` | 24 fichiers | ~15% | **50%** |
| `src/analysis/` | 12 fichiers | ~75% | **80%** |
| `src/visualization/` | 6 fichiers | ~20% | **40%** |
| **Global** | ~80 fichiers | **~41%** | **65%** |

#### Tests Existants √† Adapter (Inventaire)

**Migration Tests**

| Fichier | Tests | √Ä adapter ? |
|---------|-------|------------|
| `test_shared_schema.py` | 45 tests | ‚úÖ D√©j√† cr√©√© (Sprint 1) |
| `test_migrations.py` | 8 tests | ‚ö†Ô∏è Ajouter tests migration v4‚Üív5 |

**Sync Tests**

| Fichier | Tests | √Ä adapter ? |
|---------|-------|------------|
| `test_sync_engine.py` | 23 tests | ‚úÖ Adapter pour shared_matches |
| `test_delta_sync.py` | 12 tests | ‚úÖ Valider d√©tection matchs partag√©s |
| `test_sync_performance_score.py` | 6 tests | ‚úÖ Adapter pour batch compute |

**Repository Tests**

| Fichier | Tests | √Ä adapter ? |
|---------|-------|------------|
| `test_duckdb_repository.py` | 101 tests | ‚ö†Ô∏è **CRITIQUE** - Adapter pour ATTACH shared |
| `test_duckdb_repository_schema_contract.py` | 15 tests | ‚úÖ Valider nouveaux sch√©mas |

**UI Tests** (Smoke tests, peu nombreux actuellement)

| Fichier | Tests | √Ä cr√©er/adapter ? |
|---------|-------|-------------------|
| `test_career_page.py` | 4 tests | ‚úÖ Valider no regression |
| `test_app_phase2.py` | 8 tests | ‚úÖ Adapter pour roster complet |
| `test_teammates_refonte.py` | 12 tests | ‚úÖ Adapter pour shared data |
| `test_all_pages_v5.py` | 0 (√† cr√©er) | ‚≠ê **NOUVEAU** - Smoke tests toutes pages |

#### Nouveaux Tests √† Cr√©er

**1. Tests Migration (tests/migration/test_migration_v5.py)**

```python
def test_migrate_player_to_shared_idempotent():
    """Re-migrer un joueur ne duplique pas les donn√©es."""

def test_migrate_detects_shared_matches():
    """Migration d√©tecte et incr√©mente player_count."""

def test_migrate_preserves_data_integrity():
    """Toutes les donn√©es migr√©es sont coh√©rentes."""

def test_rollback_migration():
    """Rollback restaure l'√©tat v4 complet."""
```

**2. Tests Sync Shared (tests/test_sync_shared_v5.py)**

```python
async def test_process_known_match_saves_api_calls():
    """Match d√©j√† dans shared √©conomise 1-2 appels API."""

async def test_process_new_match_populates_shared():
    """Nouveau match ins√®re dans shared.match_registry + participants."""

async def test_parallel_api_calls():
    """Skill et events appel√©s en parall√®le (asyncio.gather)."""

async def test_batch_compute_performance_scores():
    """Calcul batch post-sync met √† jour tous les NULL."""
```

**3. Tests Repository Shared (tests/test_repository_shared_v5.py)**

```python
def test_attach_shared_matches_success():
    """ATTACH shared_matches fonctionne."""

def test_load_matches_joins_shared_and_enrichment():
    """load_matches retourne donn√©es depuis JOIN shared + enrichment."""

def test_load_participants_returns_all_players():
    """load_match_participants retourne TOUS les joueurs (8+)."""

def test_shared_db_missing_raises_error():
    """Absence de shared_matches.duckdb l√®ve une erreur explicite."""
```

**4. Tests UI (tests/ui/test_all_pages_v5.py)**

```python
@pytest.mark.parametrize("page_name", [
    "career", "match_history", "match_view", "timeseries",
    "teammates", "maps", "modes", "medals", "media_library"
])
def test_page_renders_without_error(page_name):
    """Chaque page se charge sans erreur."""

def test_match_view_displays_full_roster():
    """Match view affiche 8+ joueurs (roster complet)."""

def test_teammates_page_detects_shared_matches():
    """Page teammates d√©tecte correctement les matchs partag√©s."""
```

**5. Tests de Charge (tests/performance/test_load_v5.py)**

```python
@pytest.mark.slow
def test_load_1000_matches():
    """Repository charge 1000 matchs en < 2s."""

@pytest.mark.slow
def test_sync_500_matches():
    """Sync 500 matchs en < 15 minutes."""
```

#### Outils de Couverture

```bash
# Couverture compl√®te HTML
python -m pytest --cov=src --cov-report=html --cov-report=term-missing

# Couverture par module
python -m pytest --cov=src/data/sync --cov-report=term

# V√©rifier seuil minimal
python scripts/check_coverage_threshold.py --min 65
```

#### Estimation de Complexit√©

| T√¢che | Nb tests √† cr√©er/adapter | Temps estim√© |
|-------|-------------------------|--------------|
| Tests migration v5 | ~15 tests | 3h |
| Tests sync shared | ~20 tests | 2h |
| Tests repository shared | ~25 tests | 2h |
| Tests UI (smoke tests) | ~30 tests | 4h |
| Tests de charge | ~10 tests | 2h |
| Adapter tests existants | ~50 tests | 2h |
| Rapport couverture final | 1 rapport | 1h |
| Documentation tests | 1 fichier | 1h |

**Total** : ~17h (sur 15-17h pr√©vues)

---

### Sprint 8 ‚Äî Contexte Pr√©liminaire : Finalisation & Release

#### Code Mort √† Nettoyer (Inventaire Pr√©liminaire)

**1. VIEWs de Compatibilit√© (si cr√©√©es)**

```sql
-- √Ä supprimer si pr√©sentes dans player DBs
DROP VIEW IF EXISTS match_stats;
DROP VIEW IF EXISTS match_participants;
DROP VIEW IF EXISTS highlight_events;
DROP VIEW IF EXISTS medals_earned;
```

**Script** : `scripts/migration/remove_all_compat_views.py`

**2. Fonctions Legacy √† V√©rifier**

```python
# src/data/sync/engine.py
# Anciennes m√©thodes √† v√©rifier si encore utilis√©es :
# - _insert_match_row_v4() (si cr√©√©e pour transition)
# - _sync_without_shared() (si fallback cr√©√©)

# src/data/repositories/duckdb_repo.py
# - _load_matches_v4() (si fallback cr√©√©)
```

**3. Imports Inutilis√©s**

```bash
# D√©tecter imports inutilis√©s
ruff check src/ --select F401  # Unused imports
autoflake --remove-all-unused-imports --in-place src/**/*.py
```

**4. Code Comment√©**

```bash
# Rechercher code comment√© (√† supprimer)
grep -rn "^[[:space:]]*#.*def \|^[[:space:]]*#.*class " src/
```

**5. Archivage Documentation Temporaire (.ai/)**

```bash
# Archiver les documents de travail v5.0
mkdir -p .ai/archive/v5.0/

# Plans de projet
mv .ai/PLAN_V5_SHARED_MATCHES.md .ai/archive/v5.0/
mv .ai/PLAN_UNIFIE.md .ai/archive/v5.0/  # Ancien plan v4.5 (obsol√®te apr√®s v5)

# Rapports et analyses v5
mv .ai/v5-*.md .ai/archive/v5.0/  # Tous les docs v5 (baseline, migration, retrospective)
mv .ai/reports/v5-*.* .ai/archive/v5.0/reports/  # Rapports benchmark/coverage v5

# Garder seulement les docs actifs
# - thought_log.md (journal permanent)
# - project_map.md (cartographie permanente)
# - SPRINT_EXPLORATION.md (catalogue donn√©es)
# - *.md actifs pour v6+
```

**Script** : `scripts/archive_v5_docs.sh`

**Documents √† archiver** :
- `PLAN_V5_SHARED_MATCHES.md` (ce plan)
- `PLAN_UNIFIE.md` ‚≠ê **NOUVEAU** (ancien plan v4.5, obsol√®te)
- `v5-baseline-audit.md`
- `v5-match-overlap-analysis.md`
- `v5-migration-report.md`
- `v5-retrospective.md`
- `reports/v5-*` (tous les rapports benchmark/coverage v5)

**Documents √† conserver** :
- `thought_log.md` (journal permanent)
- `project_map.md` (mise √† jour pour v5)
- `SPRINT_EXPLORATION.md`
- `ARCHITECTURE_ROADMAP.md`
- Audits permanents (SQLITE_TO_DUCKDB_AUDIT.md, PANDAS_TO_POLARS_AUDIT.md)

**6. Archivage Scripts Sp√©cifiques v5**

```bash
# Archiver les scripts de migration v5 (usage unique)
mkdir -p scripts/_archive/migration_v5/
mv scripts/migration/create_shared_matches_db.py scripts/_archive/migration_v5/
mv scripts/migration/schema_v5.sql scripts/_archive/migration_v5/
mv scripts/migration/migrate_player_to_shared.py scripts/_archive/migration_v5/
mv scripts/migration/validate_migration.py scripts/_archive/migration_v5/
mv scripts/migration/validate_shared_schema.py scripts/_archive/migration_v5/
mv scripts/migration/create_compat_views.py scripts/_archive/migration_v5/
mv scripts/migration/remove_all_compat_views.py scripts/_archive/migration_v5/

# Archiver les scripts benchmark v5 (comparaison ponctuelle)
mkdir -p scripts/_archive/benchmark_v5/
mv scripts/benchmark_v4_vs_v5.py scripts/_archive/benchmark_v5/
mv scripts/benchmark_sync_v4_vs_v5.py scripts/_archive/benchmark_v5/
mv scripts/validate_v5_improvements.py scripts/_archive/benchmark_v5/
mv scripts/test_e2e_v5.py scripts/_archive/benchmark_v5/

# CONSERVER les scripts r√©utilisables
# - scripts/backup_player.py
# - scripts/restore_player.py
# - scripts/diagnose_player_db.py
# - scripts/sync.py
# - scripts/backfill_data.py
# - etc.
```

**Raison** : Ces scripts sont sp√©cifiques √† la migration v4‚Üív5 et n'ont plus d'utilit√© apr√®s la migration. Les archiver permet de conserver l'historique sans encombrer `scripts/` et `scripts/migration/`.

#### Documentation Obligatoire

| Document | Contenu | Statut |
|----------|---------|--------|
| `CHANGELOG.md` | Toutes les modifications v5.0 (format Keep a Changelog) | √Ä mettre √† jour |
| `README.md` | Section "Architecture v5" + gains de performance | √Ä mettre √† jour |
| `docs/ARCHITECTURE_V5.md` | Sch√©ma complet shared_matches + flux de donn√©es | √Ä cr√©er |
| `docs/MIGRATION_V4_TO_V5.md` | Guide utilisateur pour migrer de v4 √† v5 | √Ä cr√©er |
| `.ai/v5-retrospective.md` | Le√ßons apprises, difficult√©s rencontr√©es, am√©liorations futures | √Ä cr√©er |

#### Benchmark Final (Scripts √† Cr√©er)

**Script** : `scripts/benchmark_v4_vs_v5.py`

```python
def benchmark_storage():
    """Compare la taille des DBs v4 vs v5."""
    # v4 : 4 joueurs √ó 200 MB = 800 MB
    # v5 : shared (200 MB) + 4√ó30 MB = 320 MB
    # Gain : -60%

def benchmark_api_calls():
    """Compare le nombre d'appels API pour sync initiale."""
    # v4 : 4 joueurs √ó 3000 appels = 12000 appels
    # v5 : ~3300 appels (d√©tection partage)
    # Gain : -72%

def benchmark_sync_time():
    """Compare le temps de sync pour 100 matchs."""
    # v4 : ~45 minutes
    # v5 : ~12 minutes
    # Gain : -73%

def benchmark_query_performance():
    """Compare les temps de query load_matches(limit=500)."""
    # v4 : ~80ms
    # v5 : ~60ms (ATTACH optimis√©)
    # Gain : -25%
```

#### Checklist Revue de Code Compl√®te

```bash
# 1. Formatage
black src/ tests/ scripts/
isort src/ tests/ scripts/

# 2. Linting
ruff check src/ tests/ scripts/ --fix

# 3. Type checking
mypy src/ --ignore-missing-imports

# 4. Tests
python -m pytest --cov=src --cov-report=html

# 5. S√©curit√© (secrets hardcod√©s)
git secrets --scan

# 6. Benchmark
python scripts/benchmark_v4_vs_v5.py --detailed

# 7. Validation sch√©mas
python scripts/validate_all_schemas.py
```

#### Tag et Merge

```bash
# 1. V√©rifier que tous les tests passent
python -m pytest

# 2. Cr√©er le tag v5.0.0
git tag -a v5.0.0 -m "Release v5.0.0 - Shared Matches Architecture"

# 3. Push le tag
git push origin v5.0.0

# 4. Merge vers main
git checkout main
git merge v5/shared-matches-migration --no-ff
git push origin main

# 5. Cr√©er la release GitHub
gh release create v5.0.0 \
  --title "LevelUp v5.0.0 - Shared Matches Architecture" \
  --notes-file docs/RELEASE_NOTES_V5.md
```

#### Estimation de Complexit√©

| T√¢che | Temps estim√© |
|-------|--------------|
| Nettoyage code mort | 2h |
| Mise √† jour CHANGELOG.md | 1h |
| Mise √† jour README.md | 1h |
| Documentation ARCHITECTURE_V5.md | 2h |
| Documentation MIGRATION_V4_TO_V5.md | 2h |
| Benchmark final | 2h |
| Revue de code compl√®te | 3h |
| Archivage docs `.ai/` + PLAN_UNIFIE.md + scripts v5 | 45min |
| Tag + merge + release | 1h |

**Total** : ~14.75h (sur 14.5-16.5h pr√©vues)

---

## 3. Sprints D√©taill√©s

---

### Sprint 0 ‚Äî Audit Baseline & S√©curisation (1 jour) ‚úÖ

**Objectif** : √âtablir l'√©tat de r√©f√©rence et s√©curiser les donn√©es existantes

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 0.1 | ‚úÖ Cr√©er branche `v5/shared-matches-migration` depuis `sprint14/isolation-backend-frontend` | Git | 15min |
| 0.2 | ‚úÖ Backup COMPLET de toutes les DBs joueur + metadata | Scripts | 30min |
| 0.3 | ‚úÖ Exporter sch√©mas SQL actuels de toutes les DBs | `scripts/export_schemas.py` | 30min |
| 0.4 | ‚úÖ Audit des donn√©es : comptage matchs, participants, events par joueur | `scripts/audit_current_data.py` | 1h |
| 0.5 | ‚úÖ Documenter les taux de partage de matchs r√©els | `scripts/analyze_match_overlap.py` | 1h |
| 0.6 | ‚úÖ Cr√©er scripts de validation post-migration | `scripts/validate_migration.py` | 2h |
| 0.7 | ‚úÖ Tagger commit de r√©f√©rence `pre-v5-migration` | Git | 10min |

#### Livrables

- [x] Fichier `.ai/v5-baseline-audit.md` (stats compl√®tes)
- [x] Fichier `.ai/v5-match-overlap-analysis.md` (taux de partage)
- [x] Backup complet dans `backups/pre-v5-$(date)/`
- [x] Tag `pre-v5-migration` cr√©√©
- [x] Scripts de validation pr√™ts

#### Tests de Validation

```bash
# V√©rifier que les backups sont valides
python scripts/verify_backups.py backups/pre-v5-*/

# V√©rifier baseline tests
python -m pytest -q --ignore=tests/integration

# Documenter le nombre de matchs par joueur
python scripts/audit_current_data.py --summary

# Analyser les matchs partag√©s
python scripts/analyze_match_overlap.py --matrix
```

#### Gate de Livraison

- [x] Backups valid√©s (restoration test√©e sur 1 joueur)
- [x] Baseline tests passent √† 100%
- [x] Documentation baseline compl√®te
- [x] Tag `pre-v5-migration` cr√©√©

**Statut** : ‚úÖ **TERMIN√â**  
**Estimation** : 1 jour (6-7h effectives)

---

### Sprint 1 ‚Äî Infrastructure shared_matches.duckdb (2 jours) ‚úÖ

**Objectif** : Cr√©er la base de donn√©es partag√©e avec sch√©ma complet et index optimis√©s

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 1.1 | ‚úÖ Cr√©er DDL `match_registry` (table centrale) | `scripts/migration/schema_v5.sql` | 1h |
| 1.2 | ‚úÖ Cr√©er DDL `match_participants` (roster global) | Idem | 1h |
| 1.3 | ‚úÖ Cr√©er DDL `highlight_events` (events globaux) | Idem | 45min |
| 1.4 | ‚úÖ Cr√©er DDL `medals_earned` (m√©dailles tous joueurs) | Idem | 45min |
| 1.5 | ‚úÖ Cr√©er DDL `xuid_aliases` (mapping global) | Idem | 30min |
| 1.6 | ‚úÖ Cr√©er index optimis√©s (match_id, xuid, start_time) | Idem | 1h |
| 1.7 | ‚úÖ Script de cr√©ation `create_shared_matches_db.py` | `scripts/migration/` | 2h |
| 1.8 | ‚úÖ Tests unitaires du sch√©ma (contraintes, types) | `tests/migration/test_shared_schema.py` | 2h |
| 1.9 | ‚úÖ Documentation du sch√©ma (diagramme ER) | `docs/SHARED_MATCHES_SCHEMA.md` | 1h |

#### Sch√©ma SQL Principal

```sql
-- match_registry : Registre central de TOUS les matchs connus
CREATE TABLE match_registry (
    match_id VARCHAR PRIMARY KEY,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    
    -- M√©tadonn√©es du match
    playlist_id VARCHAR,
    playlist_name VARCHAR,
    map_id VARCHAR,
    map_name VARCHAR,
    pair_id VARCHAR,
    pair_name VARCHAR,
    game_variant_id VARCHAR,
    game_variant_name VARCHAR,
    mode_category VARCHAR,
    is_ranked BOOLEAN DEFAULT FALSE,
    is_firefight BOOLEAN DEFAULT FALSE,
    duration_seconds INTEGER,
    
    -- Scores des √©quipes
    team_0_score SMALLINT,
    team_1_score SMALLINT,
    
    -- M√©tadonn√©es de backfill (bitmask)
    backfill_completed INTEGER DEFAULT 0,
    participants_loaded BOOLEAN DEFAULT FALSE,
    events_loaded BOOLEAN DEFAULT FALSE,
    medals_loaded BOOLEAN DEFAULT FALSE,
    
    -- Tracking
    first_sync_by VARCHAR,              -- Gamertag du 1er joueur ayant sync ce match
    first_sync_at TIMESTAMP,
    last_updated_at TIMESTAMP,
    player_count SMALLINT DEFAULT 0,    -- Nb de joueurs track√©s ayant ce match
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_registry_time ON match_registry(start_time);
CREATE INDEX idx_registry_playlist ON match_registry(playlist_id);
CREATE INDEX idx_registry_map ON match_registry(map_id);
CREATE INDEX idx_registry_player_count ON match_registry(player_count);

-- match_participants : TOUS les joueurs de TOUS les matchs
CREATE TABLE match_participants (
    match_id VARCHAR NOT NULL,
    xuid VARCHAR NOT NULL,
    
    -- Stats du joueur dans ce match (depuis MatchStats.Players[])
    team_id INTEGER,
    outcome INTEGER,                    -- 1=Tie, 2=Win, 3=Loss, 4=Left
    rank SMALLINT,                      -- Classement dans le match
    score INTEGER,                      -- Personal score
    
    -- K/D/A
    kills SMALLINT,
    deaths SMALLINT,
    assists SMALLINT,
    
    -- Pr√©cision
    shots_fired INTEGER,
    shots_hit INTEGER,
    
    -- D√©g√¢ts
    damage_dealt FLOAT,
    damage_taken FLOAT,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (match_id, xuid),
    FOREIGN KEY (match_id) REFERENCES match_registry(match_id)
);

CREATE INDEX idx_participants_xuid ON match_participants(xuid);
CREATE INDEX idx_participants_match ON match_participants(match_id);
CREATE INDEX idx_participants_composite ON match_participants(match_id, xuid);

-- highlight_events : TOUS les √©v√©nements film√©s
CREATE SEQUENCE IF NOT EXISTS highlight_events_id_seq;
CREATE TABLE highlight_events (
    id INTEGER PRIMARY KEY DEFAULT nextval('highlight_events_id_seq'),
    match_id VARCHAR NOT NULL,
    event_type VARCHAR NOT NULL,        -- 'kill', 'death', etc.
    time_ms INTEGER,
    
    -- Identifiants
    killer_xuid VARCHAR,
    killer_gamertag VARCHAR,
    victim_xuid VARCHAR,
    victim_gamertag VARCHAR,
    
    type_hint INTEGER,
    raw_json VARCHAR,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (match_id) REFERENCES match_registry(match_id)
);

CREATE INDEX idx_events_match ON highlight_events(match_id);
CREATE INDEX idx_events_killer ON highlight_events(killer_xuid);
CREATE INDEX idx_events_victim ON highlight_events(victim_xuid);

-- medals_earned : M√©dailles de TOUS les joueurs
CREATE TABLE medals_earned (
    match_id VARCHAR NOT NULL,
    xuid VARCHAR NOT NULL,              -- De QUEL joueur
    medal_name_id INTEGER NOT NULL,
    count SMALLINT NOT NULL,
    
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (match_id, xuid, medal_name_id),
    FOREIGN KEY (match_id) REFERENCES match_registry(match_id)
);

CREATE INDEX idx_medals_match ON medals_earned(match_id);
CREATE INDEX idx_medals_xuid ON medals_earned(xuid);
CREATE INDEX idx_medals_composite ON medals_earned(match_id, xuid);

-- xuid_aliases : Mapping global xuid‚Üígamertag
CREATE TABLE xuid_aliases (
    xuid VARCHAR PRIMARY KEY,
    gamertag VARCHAR NOT NULL,
    last_seen TIMESTAMP,
    source VARCHAR,                     -- 'api', 'film', 'manual'
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_aliases_gamertag ON xuid_aliases(gamertag);
```

#### Livrables

- [x] Fichier `scripts/migration/schema_v5.sql` complet
- [x] Script `scripts/migration/create_shared_matches_db.py` fonctionnel
- [x] `data/warehouse/shared_matches.duckdb` cr√©√©e et valid√©e (via script, 45 tests passent)
- [x] Tests `tests/migration/test_shared_schema.py` passent (45/45)
- [x] Documentation `docs/SHARED_MATCHES_SCHEMA.md` compl√®te

#### Tests de Validation

```bash
# Cr√©er la DB shared_matches
python scripts/migration/create_shared_matches_db.py

# V√©rifier le sch√©ma
python -m pytest tests/migration/test_shared_schema.py -v

# Valider les contraintes et index
python scripts/migration/validate_shared_schema.py

# V√©rifier la taille (doit √™tre quasi-vide)
ls -lh data/warehouse/shared_matches.duckdb  # ~100-200 KB attendu
```

#### Gate de Livraison

- [x] `shared_matches.duckdb` cr√©√©e avec toutes les tables (6 tables)
- [x] Tous les index cr√©√©s et valid√©s (14 index)
- [x] Contraintes de cl√©s √©trang√®res actives
- [x] Tests de sch√©ma passent √† 100% (45/45)
- [x] Documentation compl√®te avec diagramme ER

**Statut** : ‚úÖ **TERMIN√â** ‚Äî Commit `980df98`  
**Estimation** : 2 jours (11-13h effectives)

---

### Sprint 2 ‚Äî Migration des Donn√©es (3 jours) ‚úÖ TERMIN√â

**Objectif** : Migrer les donn√©es des 4 joueurs vers `shared_matches.duckdb` avec validation incr√©mentale

#### Strat√©gie

Migration **s√©quentielle** avec validation √† chaque joueur :

1. **Chocoboflor** (base de r√©f√©rence, ~1000 matchs)
2. **Madina97294** (95% partag√©s ‚Üí ~50 nouveaux matchs)
3. **JGtm** (75% partag√©s ‚Üí ~250 nouveaux matchs)
4. **xxdameongamerxx** (100% partag√©s ‚Üí ~0 nouveaux matchs)

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 2.1 | ‚úÖ Script de migration g√©n√©rique `migrate_player_to_shared.py` | `scripts/migration/` | 3h |
| 2.2 | ‚úÖ Fonction `extract_all_medals()` (TOUS les joueurs) | `src/data/sync/transformers.py` | 2h |
| 2.3 | ‚úÖ Migration Chocoboflor + validation | Script | 2h |
| 2.4 | ‚úÖ Migration Madina97294 + validation taux partage | Script | 1.5h |
| 2.5 | ‚úÖ Migration JGtm + validation | Script | 1.5h |
| 2.6 | ‚úÖ Migration XxDaemonGamerxX + validation 100% partage | Script | 1h |
| 2.7 | ‚úÖ Validation crois√©e (coh√©rence des donn√©es) | Script int√©gr√© | 2h |
| 2.8 | ‚úÖ Audit post-migration (comptage, doublons, orphelins) | Script int√©gr√© | 1h |
| 2.9 | ‚úÖ Cr√©ation VIEWs de compatibilit√© dans player DBs | `scripts/migration/create_compat_views.py` | 2h |

#### Script de Migration Principal

```python
# scripts/migration/migrate_player_to_shared.py
"""
Migre les donn√©es d'un joueur vers shared_matches.duckdb.

Logique :
1. Lire tous les matchs de data/players/{gamertag}/stats.duckdb
2. Pour chaque match :
   - Si match_id existe dans shared.match_registry :
     ‚Üí Incr√©menter player_count
   - Sinon :
     ‚Üí Ins√©rer dans match_registry
     ‚Üí Ins√©rer roster complet (match_participants)
     ‚Üí Ins√©rer events (highlight_events)
     ‚Üí Ins√©rer m√©dailles de TOUS (medals_earned)
     ‚Üí Marquer first_sync_by = gamertag
"""

import duckdb
import polars as pl
from pathlib import Path
from datetime import datetime, timezone

def migrate_player_to_shared(
    gamertag: str,
    player_db_path: Path,
    shared_db_path: Path = Path("data/warehouse/shared_matches.duckdb"),
    *,
    dry_run: bool = False,
    verbose: bool = True,
) -> dict:
    """Migre un joueur vers shared_matches."""
    
    stats = {
        "matches_processed": 0,
        "matches_new": 0,
        "matches_existing": 0,
        "participants_inserted": 0,
        "events_inserted": 0,
        "medals_inserted": 0,
    }
    
    conn_player = duckdb.connect(str(player_db_path), read_only=True)
    conn_shared = duckdb.connect(str(shared_db_path), read_only=dry_run)
    
    try:
        # 1. Charger tous les matchs du joueur
        matches_df = conn_player.execute("""
            SELECT 
                match_id, start_time, end_time,
                playlist_id, playlist_name,
                map_id, map_name,
                pair_id, pair_name,
                game_variant_id, game_variant_name,
                mode_category, is_ranked, is_firefight,
                time_played_seconds as duration_seconds,
                my_team_score as team_0_score,
                enemy_team_score as team_1_score
            FROM match_stats
            ORDER BY start_time ASC
        """).pl()
        
        for match_row in matches_df.iter_rows(named=True):
            match_id = match_row['match_id']
            stats["matches_processed"] += 1
            
            # 2. V√©rifier si match existe dans shared
            exists = conn_shared.execute(
                "SELECT 1 FROM match_registry WHERE match_id = ?",
                (match_id,)
            ).fetchone()
            
            if exists:
                # Match d√©j√† migr√© par un autre joueur
                if not dry_run:
                    conn_shared.execute("""
                        UPDATE match_registry 
                        SET player_count = player_count + 1,
                            last_updated_at = CURRENT_TIMESTAMP
                        WHERE match_id = ?
                    """, (match_id,))
                stats["matches_existing"] += 1
                
                if verbose:
                    print(f"  ‚úì {match_id} (d√©j√† connu)")
                    
            else:
                # Nouveau match ‚Üí ins√©rer toutes les donn√©es
                stats["matches_new"] += 1
                
                if not dry_run:
                    # 2a. Ins√©rer dans match_registry
                    conn_shared.execute("""
                        INSERT INTO match_registry (
                            match_id, start_time, end_time,
                            playlist_id, playlist_name,
                            map_id, map_name,
                            pair_id, pair_name,
                            game_variant_id, game_variant_name,
                            mode_category, is_ranked, is_firefight,
                            duration_seconds,
                            team_0_score, team_1_score,
                            first_sync_by, first_sync_at, player_count,
                            participants_loaded, events_loaded, medals_loaded
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 1, TRUE, TRUE, TRUE)
                    """, (
                        match_id,
                        match_row['start_time'],
                        match_row['end_time'],
                        match_row['playlist_id'],
                        match_row['playlist_name'],
                        match_row['map_id'],
                        match_row['map_name'],
                        match_row['pair_id'],
                        match_row['pair_name'],
                        match_row['game_variant_id'],
                        match_row['game_variant_name'],
                        match_row['mode_category'],
                        match_row['is_ranked'],
                        match_row['is_firefight'],
                        match_row['duration_seconds'],
                        match_row['team_0_score'],
                        match_row['team_1_score'],
                        gamertag,
                        datetime.now(timezone.utc),
                    ))
                    
                    # 2b. Copier match_participants
                    participants_df = conn_player.execute(
                        "SELECT * FROM match_participants WHERE match_id = ?",
                        (match_id,)
                    ).pl()
                    
                    if not participants_df.is_empty():
                        # Ins√©rer en batch via Polars ‚Üí DuckDB
                        conn_shared.execute(
                            "INSERT INTO match_participants SELECT * FROM participants_df"
                        )
                        stats["participants_inserted"] += len(participants_df)
                    
                    # 2c. Copier highlight_events
                    events_df = conn_player.execute(
                        "SELECT match_id, event_type, time_ms, killer_xuid, killer_gamertag, victim_xuid, victim_gamertag, type_hint, raw_json FROM highlight_events WHERE match_id = ?",
                        (match_id,)
                    ).pl()
                    
                    if not events_df.is_empty():
                        conn_shared.execute(
                            "INSERT INTO highlight_events (match_id, event_type, time_ms, killer_xuid, killer_gamertag, victim_xuid, victim_gamertag, type_hint, raw_json) SELECT * FROM events_df"
                        )
                        stats["events_inserted"] += len(events_df)
                    
                    # 2d. Copier medals_earned (ATTENTION : anciennes DB n'ont que 1 joueur)
                    # Pour la migration, on extrait TOUTES les m√©dailles depuis les participants
                    # Mais on ne peut pas les avoir r√©troactivement sans re-sync
                    # Donc on copie ce qu'on a (1 joueur) et on marquera medals_loaded=PARTIAL
                    medals_df = conn_player.execute(
                        "SELECT * FROM medals_earned WHERE match_id = ?",
                        (match_id,)
                    ).pl()
                    
                    if not medals_df.is_empty():
                        # Ajouter la colonne xuid si absente (anciennes DBs)
                        if 'xuid' not in medals_df.columns:
                            # R√©cup√©rer le xuid du joueur depuis player_match_stats
                            xuid_row = conn_player.execute(
                                "SELECT xuid FROM player_match_stats LIMIT 1"
                            ).fetchone()
                            if xuid_row:
                                medals_df = medals_df.with_columns(
                                    pl.lit(xuid_row[0]).alias('xuid')
                                )
                        
                        conn_shared.execute(
                            "INSERT INTO medals_earned SELECT * FROM medals_df"
                        )
                        stats["medals_inserted"] += len(medals_df)
                
                if verbose:
                    print(f"  ‚≠ê {match_id} (nouveau match migr√©)")
        
        if not dry_run:
            conn_shared.commit()
        
    finally:
        conn_player.close()
        conn_shared.close()
    
    return stats
```

#### Ordre de Migration

```bash
# 1. Chocoboflor (base de r√©f√©rence)
python scripts/migration/migrate_player_to_shared.py Chocoboflor --verbose

# 2. Valider Chocoboflor
python scripts/migration/validate_migration.py Chocoboflor

# 3. Madina97294
python scripts/migration/migrate_player_to_shared.py Madina97294 --verbose

# 4. Valider taux de partage (doit √™tre ~95%)
python scripts/migration/validate_overlap.py Madina97294 --expected-overlap 0.95

# 5. JGtm
python scripts/migration/migrate_player_to_shared.py JGtm --verbose

# 6. xxdameongamerxx
python scripts/migration/migrate_player_to_shared.py xxdameongamerxx --verbose

# 7. Validation globale
python scripts/migration/validate_all_migrations.py
```

#### Validation Post-Migration

```sql
-- Statistiques globales
SELECT 
    COUNT(*) as total_matches,
    SUM(player_count) as total_participations,
    AVG(player_count) as avg_players_per_match,
    SUM(CASE WHEN player_count > 1 THEN 1 ELSE 0 END) as shared_matches,
    SUM(CASE WHEN player_count = 1 THEN 1 ELSE 0 END) as unique_matches
FROM match_registry;

-- R√©sultat attendu :
-- total_matches: ~1050 (vs 4000 dupliqu√©s avant)
-- avg_players_per_match: ~3.8
-- shared_matches: ~950 (90% partag√©s)
-- unique_matches: ~100

-- V√©rifier l'int√©grit√© r√©f√©rentielle
SELECT 
    COUNT(*) as orphan_participants
FROM match_participants p
LEFT JOIN match_registry r ON p.match_id = r.match_id
WHERE r.match_id IS NULL;
-- Doit retourner 0

-- V√©rifier les m√©dailles
SELECT 
    COUNT(DISTINCT match_id) as matches_with_medals,
    COUNT(*) as total_medal_records,
    COUNT(DISTINCT xuid) as unique_players_with_medals
FROM medals_earned;
```

#### Livrables

- [x] Script `migrate_player_to_shared.py` complet et test√©
- [x] Fonction `extract_all_medals()` dans `transformers.py`
- [x] Chocoboflor migr√© et valid√©
- [x] Madina97294 migr√© (22% partage ‚Äî 161 matchs communs)
- [x] JGtm migr√©
- [x] XxDaemonGamerxX migr√© (100% partage valid√©)
- [x] VIEWs de compatibilit√© cr√©√©es (20/20)
- [x] Rapport de migration `.ai/v5-migration-report.md`

#### Tests de Validation

```bash
# Test migration sur Chocoboflor
python scripts/migration/migrate_player_to_shared.py Chocoboflor --dry-run
python scripts/migration/migrate_player_to_shared.py Chocoboflor --verbose

# Validation donn√©es
python scripts/migration/validate_migration.py Chocoboflor

# Migration compl√®te
bash scripts/migration/migrate_all_players.sh

# Validation finale
python -m pytest tests/migration/test_migration_integrity.py -v
```

#### Gate de Livraison

- [x] 4 joueurs migr√©s sans erreur
- [x] Taux de partage valid√© (22.1% ‚Äî 285 matchs partag√©s sur 1289)
- [x] 0 orphelins (int√©grit√© assur√©e par logique de migration, FK retir√©es)
- [x] Comptage matchs coh√©rent (1004√ó1p, 129√ó2p, 138√ó3p, 18√ó4p)
- [x] VIEWs de compatibilit√© fonctionnelles (20/20)
- [x] Tests d'int√©grit√© passent √† 100% (25/25)

**Estimation** : 3 jours (18-20h effectives)

---

### Sprint 3 ‚Äî Refactoring Sync Engine (3 jours)

**Objectif** : Adapter `DuckDBSyncEngine` pour d√©tecter et exploiter les matchs partag√©s

#### Strat√©gie

1. **D√©tection des matchs connus** via `match_registry`
2. **Sync all√©g√©e** pour matchs existants (seulement stats perso)
3. **Sync compl√®te** pour nouveaux matchs (tout dans shared)
4. **Extraction collective** des m√©dailles (tous les joueurs)

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 3.1 | Ajouter `shared_db_path` param √† `DuckDBSyncEngine.__init__` | `src/data/sync/engine.py` | 30min |
| 3.2 | M√©thode `_get_shared_connection()` | Idem | 30min |
| 3.3 | Refactoring `_process_single_match()` : d√©tection shared | Idem | 2h |
| 3.4 | Nouvelle m√©thode `_process_known_match()` (sync all√©g√©e) | Idem | 3h |
| 3.5 | Nouvelle m√©thode `_process_new_match()` (sync compl√®te) | Idem | 3h |
| 3.6 | M√©thodes d'insertion vers shared : `_insert_to_shared_registry()` | Idem | 2h |
| 3.7 | M√©thodes d'insertion vers shared : `_insert_to_shared_participants()` | Idem | 1h |
| 3.8 | M√©thodes d'insertion vers shared : `_insert_to_shared_events()` | Idem | 1h |
| 3.9 | M√©thodes d'insertion vers shared : `_insert_to_shared_medals()` | Idem | 1h |
| 3.10 | Adapter `extract_all_medals()` pour extraire TOUS les joueurs | `src/data/sync/transformers.py` | 2h |
| 3.11 | Simplifier insertions player DB (seulement enrichment) | `src/data/sync/engine.py` | 2h |
| 3.12 | Tests unitaires du nouveau flow | `tests/test_sync_shared_matches.py` | 3h |

#### Code Principal

```python
# src/data/sync/engine.py

class DuckDBSyncEngine:
    def __init__(
        self,
        player_db_path: str | Path,
        xuid: str,
        gamertag: str,
        *,
        metadata_db_path: str | Path | None = None,
        shared_db_path: str | Path | None = None,  # ‚≠ê NOUVEAU
        tokens: Tokens | None = None,
    ):
        self._player_db_path = Path(player_db_path)
        self._xuid = xuid
        self._gamertag = gamertag
        
        # Auto-d√©tection shared_matches.duckdb
        if shared_db_path is None:
            data_dir = self._player_db_path.parent.parent.parent
            self._shared_db_path = data_dir / "warehouse" / "shared_matches.duckdb"
        else:
            self._shared_db_path = Path(shared_db_path)
        
        self._shared_connection: duckdb.DuckDBPyConnection | None = None
        # ... reste de l'init
    
    def _get_shared_connection(self) -> duckdb.DuckDBPyConnection:
        """Obtient la connexion √† shared_matches.duckdb."""
        if self._shared_connection is None:
            self._shared_connection = duckdb.connect(
                str(self._shared_db_path),
                read_only=False,
            )
            # Configuration optimale
            self._shared_connection.execute("SET enable_object_cache = true")
        return self._shared_connection
    
    async def _process_single_match(
        self,
        client: SPNKrAPIClient,
        match_id: str,
        options: SyncOptions,
    ) -> dict[str, Any]:
        """Version optimis√©e avec d√©tection des matchs partag√©s."""
        
        # 1. V√©rifier dans shared_matches
        shared_conn = self._get_shared_connection()
        registry = shared_conn.execute(
            """SELECT 
                backfill_completed, 
                participants_loaded, 
                events_loaded,
                medals_loaded,
                player_count
            FROM match_registry 
            WHERE match_id = ?""",
            (match_id,)
        ).fetchone()
        
        if registry:
            # ‚úÖ Match connu ‚Üí sync all√©g√©e
            logger.info(f"Match {match_id} d√©j√† connu (player_count={registry[4]})")
            return await self._process_known_match(
                client, match_id, registry, options
            )
        else:
            # ‚≠ê Nouveau match ‚Üí sync compl√®te
            logger.info(f"Nouveau match {match_id}")
            return await self._process_new_match(
                client, match_id, options
            )
    
    async def _process_known_match(
        self,
        client: SPNKrAPIClient,
        match_id: str,
        registry: tuple,
        options: SyncOptions,
    ) -> dict[str, Any]:
        """Traite un match d√©j√† connu (sync all√©g√©e)."""
        
        result = {
            "inserted": True,
            "mode": "known_match",
            "api_calls_saved": 0,
        }
        
        # 1. T√©l√©charger SEULEMENT les stats (pour extraire mes donn√©es perso)
        stats_json = await client.get_match_stats(match_id)
        if not stats_json:
            result["error"] = f"Impossible de r√©cup√©rer {match_id}"
            return result
        
        # 2. Extraire MES donn√©es personnelles depuis Players[]
        me = _find_player(stats_json.get("Players", []), self._xuid)
        if not me:
            result["error"] = f"Joueur {self._xuid} absent du match {match_id}"
            return result
        
        # 3. Calculer mon enrichissement personnel
        # (performance_score sera calcul√© post-sync)
        player_enrichment = {
            "match_id": match_id,
            "xuid": self._xuid,
            "performance_score": None,  # Calcul√© en batch apr√®s sync
            "session_id": None,          # Calcul√© apr√®s sync
            "session_label": None,
            "is_with_friends": False,    # TODO: d√©tecter depuis friends list
            "friends_xuids": None,
        }
        
        # 4. Ins√©rer dans player DB (seulement enrichment)
        async with self._db_lock:
            conn = self._get_connection()
            conn.execute("""
                INSERT OR REPLACE INTO player_match_enrichment
                (match_id, xuid, performance_score, session_id, session_label, is_with_friends, friends_xuids)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                player_enrichment["match_id"],
                player_enrichment["xuid"],
                player_enrichment["performance_score"],
                player_enrichment["session_id"],
                player_enrichment["session_label"],
                player_enrichment["is_with_friends"],
                player_enrichment["friends_xuids"],
            ))
        
        # 5. Backfill s√©lectif si des donn√©es manquent dans shared
        backfill_needed = []
        
        if not registry[1]:  # participants_loaded
            participants = extract_participants(stats_json)
            self._insert_to_shared_participants(match_id, participants)
            backfill_needed.append("participants")
        
        if not registry[2] and options.with_highlight_events:  # events_loaded
            events = await client.get_highlight_events(match_id)
            self._insert_to_shared_events(match_id, events)
            backfill_needed.append("events")
        else:
            result["api_calls_saved"] += 1  # On a √©vit√© l'appel /film
        
        if not registry[3]:  # medals_loaded
            medals_all = extract_all_medals(stats_json)
            self._insert_to_shared_medals(match_id, medals_all)
            backfill_needed.append("medals")
        
        # 6. Incr√©menter player_count dans shared
        shared_conn = self._get_shared_connection()
        shared_conn.execute("""
            UPDATE match_registry 
            SET player_count = player_count + 1,
                last_updated_at = CURRENT_TIMESTAMP
            WHERE match_id = ?
        """, (match_id,))
        
        if backfill_needed:
            logger.info(f"Backfill effectu√© pour {match_id}: {', '.join(backfill_needed)}")
        else:
            logger.info(f"Match {match_id} complet, aucun backfill n√©cessaire")
        
        # √âCONOMIE : 1-2 appels API √©vit√©s (events + √©ventuellement skill)
        result["api_calls_saved"] += len(backfill_needed) == 0 and 1 or 0
        
        return result
    
    async def _process_new_match(
        self,
        client: SPNKrAPIClient,
        match_id: str,
        options: SyncOptions,
    ) -> dict[str, Any]:
        """Traite un nouveau match (sync compl√®te vers shared)."""
        
        result = {
            "inserted": True,
            "mode": "new_match",
        }
        
        # 1. T√©l√©charger toutes les donn√©es
        stats_json = await client.get_match_stats(match_id)
        if not stats_json:
            result["error"] = f"Impossible de r√©cup√©rer {match_id}"
            return result
        
        # Enrichir avec les assets si demand√©
        if options.with_assets:
            await enrich_match_info_with_assets(client, stats_json)
        
        # 2. T√©l√©charger events et skill en parall√®le
        xuids = extract_xuids_from_match(stats_json)
        
        events = []
        skill_json = None
        
        if options.with_highlight_events or options.with_skill:
            tasks = []
            if options.with_highlight_events:
                tasks.append(client.get_highlight_events(match_id))
            else:
                tasks.append(asyncio.sleep(0))  # Placeholder
            
            if options.with_skill and xuids:
                tasks.append(client.get_skill_stats(match_id, xuids))
            else:
                tasks.append(asyncio.sleep(0))
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            if options.with_highlight_events:
                events = results[0] if not isinstance(results[0], Exception) else []
            if options.with_skill and xuids:
                skill_json = results[1] if not isinstance(results[1], Exception) else None
        
        # 3. Extraire les donn√©es communes
        match_common = self._extract_match_common_data(stats_json, skill_json)
        participants = extract_participants(stats_json)
        medals_all = extract_all_medals(stats_json)  # ‚≠ê TOUS les joueurs
        
        # 4. Ins√©rer dans shared_matches
        shared_conn = self._get_shared_connection()
        
        # 4a. match_registry
        shared_conn.execute("""
            INSERT INTO match_registry (
                match_id, start_time, end_time,
                playlist_id, playlist_name,
                map_id, map_name,
                pair_id, pair_name,
                game_variant_id, game_variant_name,
                mode_category, is_ranked, is_firefight,
                duration_seconds,
                team_0_score, team_1_score,
                first_sync_by, first_sync_at, player_count,
                participants_loaded, events_loaded, medals_loaded
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, 1, TRUE, ?, TRUE)
        """, (
            match_id,
            match_common["start_time"],
            match_common["end_time"],
            match_common["playlist_id"],
            match_common["playlist_name"],
            match_common["map_id"],
            match_common["map_name"],
            match_common["pair_id"],
            match_common["pair_name"],
            match_common["game_variant_id"],
            match_common["game_variant_name"],
            match_common["mode_category"],
            match_common["is_ranked"],
            match_common["is_firefight"],
            match_common["duration_seconds"],
            match_common["team_0_score"],
            match_common["team_1_score"],
            self._gamertag,
            len(events) > 0,  # events_loaded
        ))
        
        # 4b. Participants
        self._insert_to_shared_participants(match_id, participants)
        
        # 4c. Events
        if events:
            self._insert_to_shared_events(match_id, events)
        
        # 4d. M√©dailles de TOUS les joueurs
        self._insert_to_shared_medals(match_id, medals_all)
        
        # 5. Ins√©rer enrichissement personnel dans player DB
        player_enrichment = {
            "match_id": match_id,
            "xuid": self._xuid,
            "performance_score": None,
            "session_id": None,
            "session_label": None,
            "is_with_friends": False,
            "friends_xuids": None,
        }
        
        async with self._db_lock:
            conn = self._get_connection()
            conn.execute("""
                INSERT OR REPLACE INTO player_match_enrichment
                (match_id, xuid, performance_score, session_id, session_label, is_with_friends, friends_xuids)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                player_enrichment["match_id"],
                player_enrichment["xuid"],
                player_enrichment["performance_score"],
                player_enrichment["session_id"],
                player_enrichment["session_label"],
                player_enrichment["is_with_friends"],
                player_enrichment["friends_xuids"],
            ))
        
        logger.info(f"Match {match_id} enti√®rement sync vers shared_matches")
        
        return result
```

#### Livrables

- [x] `DuckDBSyncEngine` refactor√© avec d√©tection shared
- [x] M√©thodes `_process_known_match()` et `_process_new_match()`
- [x] M√©thodes d'insertion vers shared (registry, participants, events, medals, aliases)
- [x] `extract_all_medals()` dans `transformers.py` (d√©j√† impl√©ment√©)
- [x] `extract_match_registry_data()` dans `transformers.py`
- [x] Tests `tests/test_sync_shared_matches.py` passent (33/33)
- [ ] Documentation `docs/SYNC_SHARED_MATCHES.md`

#### Tests de Validation

```bash
# Test unitaire du nouveau flow
python -m pytest tests/test_sync_shared_matches.py -v

# Test end-to-end sur un joueur
python scripts/sync.py --delta --player TestPlayer --max-matches 10

# V√©rifier qu'un match partag√© √©conomise des appels API
python scripts/test_api_savings.py Chocoboflor Madina97294

# Validation compl√®te
python -m pytest tests/ -v --ignore=tests/integration
```

#### Gate de Livraison

- [x] D√©tection des matchs partag√©s fonctionne
- [x] Sync all√©g√©e √©conomise 1-2 appels API par match partag√©
- [x] Sync compl√®te ins√®re dans shared correctement
- [x] M√©dailles de TOUS les joueurs extraites
- [x] Tests passent √† 100% (76/76 : 43 v4 + 33 v5)
- [x] Aucune r√©gression sur sync existant

**Estimation** : 3 jours (20-22h effectives)

---

### Sprint 4 ‚Äî Refactoring DuckDBRepository (2 jours)

**Objectif** : Adapter `DuckDBRepository` pour lire depuis `shared_matches` via ATTACH

#### Strat√©gie

1. **ATTACH** de `shared_matches.duckdb` en lecture seule
2. **Queries natives** lisant depuis `shared.*`
3. **VIEWs temporaires** pour compatibilit√©
4. **Tests de non-r√©gression** sur toutes les pages UI

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 4.1 | Ajouter `shared_db_path` param √† `DuckDBRepository.__init__` | `src/data/repositories/duckdb_repo.py` | 30min |
| 4.2 | Modifier `_get_connection()` pour ATTACH shared_matches | Idem | 1h |
| 4.3 | Refactoring `load_match_participants()` ‚Üí lecture depuis shared | Idem | 1h |
| 4.4 | Refactoring `load_highlight_events()` ‚Üí lecture depuis shared | Idem | 1h |
| 4.5 | Refactoring `load_medals_for_match()` ‚Üí lecture depuis shared | Idem | 1h |
| 4.6 | Nouvelle m√©thode `load_player_match_enrichment()` | Idem | 1h |
| 4.7 | Adapter `load_matches()` pour JOIN shared + enrichment | Idem | 2h |
| 4.8 | Cr√©er VIEWs de compat si n√©cessaire | `scripts/create_compat_views.py` | 1h |
| 4.9 | Tests unitaires repository | `tests/test_duckdb_repository_v5.py` | 3h |
| 4.10 | Tests d'int√©gration (toutes les pages UI) | `tests/integration/test_ui_pages_v5.py` | 3h |

#### Code Principal

```python
# src/data/repositories/duckdb_repo.py

class DuckDBRepository:
    def __init__(
        self,
        player_db_path: str | Path,
        xuid: str,
        *,
        metadata_db_path: str | Path | None = None,
        shared_db_path: str | Path | None = None,  # ‚≠ê NOUVEAU
        gamertag: str | None = None,
        read_only: bool = True,
        memory_limit: str = "512MB",
    ):
        self._player_db_path = Path(player_db_path)
        self._xuid = xuid
        self._gamertag = gamertag
        self._read_only = read_only
        self._memory_limit = memory_limit
        
        # Auto-d√©tection shared_matches.duckdb
        if shared_db_path is None:
            data_dir = self._player_db_path.parent.parent.parent
            self._shared_db_path = data_dir / "warehouse" / "shared_matches.duckdb"
        else:
            self._shared_db_path = Path(shared_db_path)
        
        # ... reste init
    
    def _get_connection(self) -> duckdb.DuckDBPyConnection:
        """Connexion avec ATTACH de metadata ET shared_matches."""
        if self._connection is None:
            self._connection = duckdb.connect(
                str(self._player_db_path),
                read_only=self._read_only,
            )
            
            # ATTACH metadata (existant)
            if self._metadata_db_path.exists():
                self._connection.execute(
                    f"ATTACH DATABASE '{self._metadata_db_path}' AS meta (READ_ONLY)"
                )
                self._attached_dbs.add("meta")
            
            # ‚≠ê ATTACH shared_matches
            if self._shared_db_path.exists():
                self._connection.execute(
                    f"ATTACH DATABASE '{self._shared_db_path}' AS shared (READ_ONLY)"
                )
                self._attached_dbs.add("shared")
            
            # Configuration optimale
            self._connection.execute("SET enable_object_cache = true")
            self._connection.execute(f"SET memory_limit = '{self._memory_limit}'")
        
        return self._connection
    
    def load_matches(
        self,
        *,
        limit: int | None = None,
        offset: int = 0,
        filters: dict | None = None,
    ) -> pl.DataFrame:
        """Charge les matchs avec JOIN shared + enrichment."""
        
        conn = self._get_connection()
        
        # ‚≠ê JOIN entre shared.match_participants et player_match_enrichment
        query = """
            SELECT 
                -- Donn√©es communes depuis shared.match_participants
                p.match_id,
                p.xuid,
                p.team_id,
                p.outcome,
                p.rank,
                p.score AS personal_score,
                p.kills,
                p.deaths,
                p.assists,
                p.shots_fired,
                p.shots_hit,
                CASE 
                    WHEN p.shots_fired > 0 
                    THEN (p.shots_hit * 100.0 / p.shots_fired)
                    ELSE 0 
                END AS accuracy,
                p.damage_dealt,
                p.damage_taken,
                
                -- M√©tadonn√©es depuis shared.match_registry
                r.start_time,
                r.end_time,
                r.playlist_id,
                r.playlist_name,
                r.map_id,
                r.map_name,
                r.mode_category,
                r.is_ranked,
                r.team_0_score,
                r.team_1_score,
                r.duration_seconds,
                
                -- Enrichissement personnel depuis player DB
                e.performance_score,
                e.session_id,
                e.session_label,
                e.is_with_friends
                
            FROM shared.match_participants p
            INNER JOIN shared.match_registry r ON r.match_id = p.match_id
            LEFT JOIN player_match_enrichment e ON e.match_id = p.match_id
            WHERE p.xuid = ?
        """
        
        params = [self._xuid]
        
        # Filtres (existant, √† adapter)
        if filters:
            where_clauses = []
            # ... logique filtres adapt√©e aux nouvelles colonnes
        
        query += " ORDER BY r.start_time DESC"
        
        if limit:
            query += f" LIMIT {limit} OFFSET {offset}"
        
        return conn.execute(query, params).pl()
    
    def load_match_participants(
        self,
        match_id: str,
    ) -> pl.DataFrame:
        """Charge le roster complet depuis shared.match_participants."""
        
        conn = self._get_connection()
        
        # ‚≠ê Lecture directe depuis shared
        return conn.execute("""
            SELECT 
                p.match_id,
                p.xuid,
                p.team_id,
                p.outcome,
                p.rank,
                p.score,
                p.kills,
                p.deaths,
                p.assists,
                p.shots_fired,
                p.shots_hit,
                p.damage_dealt,
                p.damage_taken,
                COALESCE(a.gamertag, 'Unknown') as gamertag
            FROM shared.match_participants p
            LEFT JOIN shared.xuid_aliases a ON a.xuid = p.xuid
            WHERE p.match_id = ?
            ORDER BY p.rank ASC
        """, (match_id,)).pl()
    
    def load_highlight_events(
        self,
        match_id: str,
    ) -> pl.DataFrame:
        """Charge les events depuis shared.highlight_events."""
        
        conn = self._get_connection()
        
        return conn.execute("""
            SELECT * FROM shared.highlight_events
            WHERE match_id = ?
            ORDER BY time_ms ASC
        """, (match_id,)).pl()
    
    def load_medals_for_match(
        self,
        match_id: str,
        xuid: str | None = None,
    ) -> pl.DataFrame:
        """Charge les m√©dailles depuis shared.medals_earned."""
        
        conn = self._get_connection()
        
        if xuid is None:
            xuid = self._xuid
        
        # ‚≠ê Filtrer par match_id ET xuid
        return conn.execute("""
            SELECT 
                m.match_id,
                m.xuid,
                m.medal_name_id,
                m.count,
                md.name_fr,
                md.description_fr,
                md.difficulty
            FROM shared.medals_earned m
            LEFT JOIN meta.medal_definitions md ON md.name_id = m.medal_name_id
            WHERE m.match_id = ? AND m.xuid = ?
        """, (match_id, xuid)).pl()
```

#### Livrables

- [ ] `DuckDBRepository` refactor√© avec ATTACH shared
- [ ] Toutes les m√©thodes de lecture adapt√©es
- [ ] VIEWs de compatibilit√© si n√©cessaire
- [ ] Tests `tests/test_duckdb_repository_v5.py` passent
- [ ] Tests d'int√©gration UI passent

#### Tests de Validation

```bash
# Tests repository
python -m pytest tests/test_duckdb_repository_v5.py -v

# Tests d'int√©gration (toutes les pages)
python -m pytest tests/integration/test_ui_pages_v5.py -v

# Test manuel UI
streamlit run streamlit_app.py

# Valider queries
python scripts/validate_repository_queries.py
```

#### Gate de Livraison

- [ ] ATTACH shared_matches fonctionne
- [ ] Queries depuis shared correctes
- [ ] Aucune r√©gression UI
- [ ] Performance acceptable (< 100ms par query)
- [ ] Tests passent √† 100%

**Estimation** : 2 jours (13-15h effectives)

---

### Sprint 5 ‚Äî Refactoring UI Big Bang (3 jours)

**Objectif** : Supprimer les VIEWs de compatibilit√© et adapter toutes les pages UI pour queries natives

#### Strat√©gie

**Big Bang** : Refactorer toutes les pages en une fois pour √©viter le code hybride

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 5.1 | Audit de toutes les queries UI (inventaire) | `scripts/audit_ui_queries.py` | 1h |
| 5.2 | Refactoring page Career | `src/ui/pages/career.py` | 2h |
| 5.3 | Refactoring page Match History | `src/ui/pages/match_history.py` | 2h |
| 5.4 | Refactoring page Match View | `src/ui/pages/match_view.py` | 2h |
| 5.5 | Refactoring page Timeseries | `src/ui/pages/timeseries.py` | 2h |
| 5.6 | Refactoring page Teammates | `src/ui/pages/teammates.py` | 2h |
| 5.7 | Refactoring page Maps | `src/ui/pages/maps.py` | 1.5h |
| 5.8 | Refactoring page Modes | `src/ui/pages/modes.py` | 1.5h |
| 5.9 | Refactoring page Medals | `src/ui/pages/medals.py` | 1.5h |
| 5.10 | Refactoring page Media Library | `src/ui/pages/media_library.py` | 2h |
| 5.11 | Suppression VIEWs de compatibilit√© | `scripts/remove_compat_views.py` | 1h |
| 5.12 | Tests automatis√©s toutes les pages | `tests/ui/test_all_pages_v5.py` | 4h |

#### Exemple de Refactoring

```python
# AVANT (v4)
def load_match_data(repo, match_id):
    # Query attendait que match_stats contienne tout
    df = repo.load_matches(filters={"match_id": match_id})
    return df

# APR√àS (v5)
def load_match_data(repo, match_id):
    # Donn√©es depuis shared.match_participants + enrichment
    df = repo.load_matches(filters={"match_id": match_id})
    # La query a chang√© en interne (JOIN shared), mais l'API reste identique
    return df
```

#### Livrables

- [ ] Toutes les pages UI refactor√©es
- [ ] VIEWs de compatibilit√© supprim√©es
- [ ] Tests UI passent √† 100%
- [ ] Guide de migration UI `.ai/v5-ui-migration-guide.md`

#### Tests de Validation

```bash
# Test toutes les pages
python -m pytest tests/ui/test_all_pages_v5.py -v

# Test manuel streamlit
streamlit run streamlit_app.py

# V√©rifier aucune r√©gression
python scripts/test_ui_regression.py
```

#### Gate de Livraison

- [ ] Toutes les pages fonctionnent
- [ ] Aucune r√©gression visuelle
- [ ] Performance acceptable
- [ ] Tests UI passent √† 100%

**Estimation** : 3 jours (20-22h effectives)

---

### Sprint 6 ‚Äî Optimisation API (2 jours)

**Objectif** : Impl√©menter les optimisations du PLAN_OPTIMISATION_SYNC.md

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 6.1 | Parall√©liser appels API skill+events (`asyncio.gather`) | `src/data/sync/engine.py` | 2h |
| 6.2 | D√©sactiver calcul performance_score pendant sync | `src/data/sync/engine.py` | 1h |
| 6.3 | Cr√©er `batch_compute_performance_scores()` post-sync | `src/data/sync/engine.py` | 3h |
| 6.4 | Batching des insertions DB (commit tous les 10 matchs) | `src/data/sync/engine.py` | 2h |
| 6.5 | Augmenter rate limit (10 req/s, parallel_matches=5) | `src/data/sync/models.py` | 30min |
| 6.6 | Tests de performance (benchmark) | `tests/performance/test_sync_v5.py` | 2h |
| 6.7 | Documentation optimisations | `docs/SYNC_OPTIMIZATIONS_V5.md` | 1h |

#### Gains Attendus

| M√©trique | Avant v5 | Apr√®s v5 | Gain |
|----------|----------|----------|------|
| Temps/match (nouveau) | 16s | 2-3s | **-81%** |
| Temps/match (partag√© 95%) | 16s | 0.5s | **-97%** |
| API calls (sync 4 joueurs) | 12 000 | 3 300 | **-72%** |

#### Livrables

- [ ] Parall√©lisation API impl√©ment√©e
- [ ] Perf scores calcul√©s en batch post-sync
- [ ] Batching DB impl√©ment√©
- [ ] Rate limit optimis√©
- [ ] Tests de performance valid√©s
- [ ] Documentation compl√®te

#### Tests de Validation

```bash
# Benchmark sync
python tests/performance/test_sync_v5.py --benchmark

# Comparaison v4 vs v5
python scripts/benchmark_sync_v4_vs_v5.py

# Validation gains
python scripts/validate_optimizations.py
```

#### Gate de Livraison

- [ ] Temps/match < 3s (nouveaux matchs)
- [ ] Temps/match < 1s (matchs partag√©s)
- [ ] Aucune r√©gression de donn√©es
- [ ] Tests passent √† 100%

**Estimation** : 2 jours (11-13h effectives)

---

### Sprint 7 ‚Äî Tests & Couverture (2 jours)

**Objectif** : Atteindre 80% de couverture et impl√©menter PLAN_AMELIORATION_TESTS.md

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 7.1 | Tests migration (int√©grit√©, rollback) | `tests/migration/` | 3h |
| 7.2 | Tests sync shared (d√©tection, √©conomies API) | `tests/test_sync_shared_v5.py` | 2h |
| 7.3 | Tests repository shared (ATTACH, queries) | `tests/test_repository_shared_v5.py` | 2h |
| 7.4 | Tests UI (toutes les pages, edge cases) | `tests/ui/` | 4h |
| 7.5 | Tests de charge (1000+ matchs) | `tests/performance/test_load_v5.py` | 2h |
| 7.6 | Rapport de couverture final | Coverage | 1h |
| 7.7 | Documentation tests | `docs/TESTING_V5.md` | 1h |

#### Couverture Cible

| Module | Cible | Actuel v4 | Objectif v5 |
|--------|-------|-----------|-------------|
| `src/data/sync/` | 80% | 65% | **85%** |
| `src/data/repositories/` | 80% | 70% | **85%** |
| `src/ui/pages/` | 40% | 15% | **50%** |
| Global | 60% | 41% | **65%** |

#### Livrables

- [ ] Couverture >= 65% globale
- [ ] Tests migration √† 100%
- [ ] Tests UI >= 50%
- [ ] Rapport de couverture dans `.ai/v5-coverage-report.html`

#### Tests de Validation

```bash
# Suite compl√®te avec couverture
python -m pytest --cov=src --cov-report=html --cov-report=term-missing

# V√©rifier couverture minimale
python scripts/check_coverage_threshold.py --min 65

# Tests de charge
python -m pytest tests/performance/test_load_v5.py -v
```

#### Gate de Livraison

- [ ] Couverture >= 65%
- [ ] Tous les tests passent
- [ ] Aucun test ignor√© sans justification
- [ ] Documentation tests compl√®te

**Estimation** : 2 jours (15-17h effectives)

---

### Sprint 8 ‚Äî Finalisation & Release v5.0 (2 jours)

**Objectif** : Stabilisation, documentation, et release officielle v5.0

#### T√¢ches

| # | T√¢che | Fichier(s) | Dur√©e |
|---|-------|-----------|-------|
| 8.1 | Nettoyage code mort (VIEWs, legacy) | Divers | 2h |
| 8.2 | Mise √† jour CHANGELOG.md | `CHANGELOG.md` | 1h |
| 8.3 | Mise √† jour README.md | `README.md` | 1h |
| 8.4 | Documentation architecture v5 | `docs/ARCHITECTURE_V5.md` | 2h |
| 8.5 | Guide de migration v4‚Üív5 | `docs/MIGRATION_V4_TO_V5.md` | 2h |
| 8.6 | Benchmark final (comparaison v4 vs v5) | `scripts/benchmark_v4_vs_v5.py` | 2h |
| 8.7 | Revue de code compl√®te | Tous | 3h |
| 8.8 | Archivage docs `.ai/` + PLAN_UNIFIE.md + scripts v5 | `scripts/archive_v5_all.sh` | 45min |
| 8.9 | Tag `v5.0.0` et merge vers `main` | Git | 1h |

#### Documentation Obligatoire

- [ ] `CHANGELOG.md` √† jour
- [ ] `README.md` mis √† jour
- [ ] `docs/ARCHITECTURE_V5.md` complet
- [ ] `docs/MIGRATION_V4_TO_V5.md` d√©taill√©
- [ ] `.ai/v5-retrospective.md` (le√ßons apprises)

#### Benchmark Final

| M√©trique | v4 | v5 | Am√©lioration |
|----------|----|----|--------------|
| **Stockage** (4 joueurs) | 800 MB | 250 MB | **-69%** |
| **Sync initiale** (4 joueurs) | 12 000 appels | 3 300 appels | **-72%** |
| **Temps sync** (100 matchs) | 45 min | 12 min | **-73%** |
| **DB size** (par joueur) | 200 MB | 30 MB | **-85%** |

#### Livrables

- [ ] Code nettoy√© et optimis√©
- [ ] Documentation compl√®te
- [ ] Benchmark valid√©
- [ ] Documentation temporaire `.ai/` archiv√©e dans `.ai/archive/v5.0/`
- [ ] `PLAN_UNIFIE.md` archiv√© (ancien plan v4.5 obsol√®te)
- [ ] Scripts migration v5 archiv√©s dans `scripts/_archive/migration_v5/`
- [ ] Scripts benchmark v5 archiv√©s dans `scripts/_archive/benchmark_v5/`
- [ ] Tag `v5.0.0` cr√©√©
- [ ] Merge vers `main` effectu√©

#### Tests de Validation

```bash
# Suite compl√®te finale
python -m pytest --cov=src --cov-report=html

# Benchmark comparatif
python scripts/benchmark_v4_vs_v5.py --detailed

# Validation gains
python scripts/validate_v5_improvements.py

# Tests end-to-end
python scripts/test_e2e_v5.py
```

#### Gate de Livraison

- [ ] Tous les tests passent √† 100%
- [ ] Couverture >= 65%
- [ ] Benchmark valid√© (gains >= objectifs)
- [ ] Documentation compl√®te
- [ ] Aucun `# TODO` ou `# FIXME` sans ticket
- [ ] Dossier `.ai/` nettoy√© (docs v5 + PLAN_UNIFIE.md archiv√©s)
- [ ] Scripts v5 archiv√©s (migration + benchmark)
- [ ] Tag `v5.0.0` cr√©√©
- [ ] Merge vers `main` effectu√©

**Estimation** : 2 jours (14.75-16.75h effectives)

---

## 4. Protocole de Revue par Sprint

### 4.1 Checklist Obligatoire Pr√©-Commit

**√Ä ex√©cuter AVANT CHAQUE commit** :

```bash
# 1. Tests locaux
python -m pytest -q --ignore=tests/integration

# 2. Type hints
mypy src/ --ignore-missing-imports

# 3. Formatage
black src/ tests/ scripts/
isort src/ tests/ scripts/

# 4. Linting
ruff check src/ tests/ scripts/

# 5. Validation sch√©ma (si modif DB)
python scripts/validate_all_schemas.py
```

### 4.2 Checklist de Fin de Sprint

- [ ] **Tests** : `pytest` passe √† 100%
- [ ] **Couverture** : Pas de baisse de couverture (minimum maintenu)
- [ ] **Documentation** : README/CHANGELOG/docs √† jour
- [ ] **Git** : Commits propres avec messages Conventional Commits
- [ ] **Backups** : Backups de DB cr√©√©s si modifications de sch√©ma
- [ ] **Validation** : Tests manuels sur UI (smoke test)
- [ ] **Performance** : Pas de r√©gression (bench si pertinent)
- [ ] **Code mort** : Aucun code comment√© ou inutilis√©

### 4.3 Validation Inter-Sprints

Avant de passer au sprint suivant :

1. **Revue auto-critique** : Relire son propre code 24h apr√®s
2. **Tests de r√©gression** : Suite compl√®te `pytest`
3. **Validation UI** : Tester manuellement les pages critiques
4. **Documentation** : V√©rifier que tout est √† jour
5. **Git** : Cr√©er un tag de checkpoint `sprint-N-completed`

---

## 5. Matrice de Risques

### 5.1 Risques Techniques

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Corruption shared_matches.duckdb** | Faible | Critique | Backups quotidiens + validation checksums |
| **Perte de donn√©es lors migration** | Faible | Critique | Migration incr√©mentale + validation √† chaque joueur |
| **R√©gression UI** | Moyen | √âlev√© | Tests UI complets + validation manuelle |
| **Performance queries d√©grad√©es** | Moyen | Moyen | Benchmark avant/apr√®s + index optimis√©s |
| **Incompatibilit√© ATTACH multi-DB** | Faible | Moyen | Tests DuckDB version 1.4.4+ valid√©s |

### 5.2 Risques Op√©rationnels

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| **Rollback impossible** | Faible | Critique | Backups complets + plan de rollback test√© |
| **Downtime prolong√©** | Moyen | Moyen | Migration par joueur (autres accessibles) |
| **Bugs non d√©tect√©s** | Moyen | Moyen | Couverture tests >= 65% + tests manuels |

---

## 6. Crit√®res de Livraison Globaux

### 6.1 Crit√®res Fonctionnels

- [ ] ‚úÖ Tous les joueurs migr√©s vers shared_matches
- [ ] ‚úÖ Aucune duplication de donn√©es de matchs
- [ ] ‚úÖ D√©tection des matchs partag√©s fonctionnelle
- [ ] ‚úÖ Sync all√©g√©e pour matchs connus
- [ ] ‚úÖ Toutes les pages UI fonctionnelles
- [ ] ‚úÖ Aucune r√©gression de features

### 6.2 Crit√®res Techniques

- [ ] ‚úÖ Tests passent √† 100% (`pytest`)
- [ ] ‚úÖ Couverture >= 65%
- [ ] ‚úÖ Aucun warning ou erreur lors du build
- [ ] ‚úÖ Type hints complets sur code m√©tier
- [ ] ‚úÖ Documentation √† jour (README, CHANGELOG, docs/)

### 6.3 Crit√®res de Performance

- [ ] ‚úÖ Stockage r√©duit de >= 65%
- [ ] ‚úÖ Appels API r√©duits de >= 70%
- [ ] ‚úÖ Temps de sync r√©duit de >= 70%
- [ ] ‚úÖ Queries UI < 100ms (p95)

### 6.4 Crit√®res Qualit√©

- [ ] ‚úÖ Code modulaire (max 500 lignes/module)
- [ ] ‚úÖ Pas de duplication de logique
- [ ] ‚úÖ Gestion d'erreurs exhaustive
- [ ] ‚úÖ Logs clairs et exploitables
- [ ] ‚úÖ Aucun `# TODO` sans ticket associ√©

---

## 7. Plan de Rollback

### 7.1 Si √âchec Critique D√©tect√©

**Crit√®res de rollback** :
- Perte de donn√©es > 1%
- Corruption de shared_matches.duckdb irr√©parable
- R√©gression UI bloquante (> 50% des pages cass√©es)
- Performance < -50% vs v4

**Proc√©dure** :

```bash
# 1. Arr√™ter toutes les op√©rations
git stash

# 2. Restaurer depuis backup
python scripts/restore_all_from_backup.py backups/pre-v5-*/

# 3. Retour branche pr√©c√©dente
git checkout sprint14/isolation-backend-frontend

# 4. V√©rifier l'√©tat
python -m pytest -q
streamlit run streamlit_app.py

# 5. Documenter l'incident
# Cr√©er .ai/v5-rollback-incident-$(date).md
```

### 7.2 Sauvegarde Continue

Pendant toute la migration :

- **Checkpoints Git** : Tag apr√®s chaque sprint
- **Backups DB** : Quotidiens dans `backups/v5-daily/`
- **Logs d√©taill√©s** : Journalisation de toutes les op√©rations

---

## 8. M√©triques de Succ√®s

### 8.1 M√©triques Primaires

| M√©trique | Objectif | Mesure |
|----------|----------|--------|
| **R√©duction stockage** | >= 65% | (v4_size - v5_size) / v4_size |
| **R√©duction API calls** | >= 70% | (v4_calls - v5_calls) / v4_calls |
| **R√©duction temps sync** | >= 70% | (v4_time - v5_time) / v4_time |
| **Couverture tests** | >= 65% | Coverage report |
| **Tests passants** | 100% | Pytest exit code |

### 8.2 M√©triques Secondaires

| M√©trique | Objectif | Mesure |
|----------|----------|--------|
| **Taille player DB** | < 50 MB | `du -sh data/players/*/stats.duckdb` |
| **Temps query UI** | < 100ms (p95) | Benchmark Streamlit |
| **Partage de matchs d√©tect√©** | >= 90% | Statistiques match_registry |

---

## 9. R√©capitulatif Timeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    LevelUp v5.0 Timeline                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Sprint 0    ‚îÇ Audit & Backups                         (1j)  ‚îÇ
‚îÇ Sprint 1    ‚îÇ Infrastructure shared_matches           (2j)  ‚îÇ
‚îÇ Sprint 2    ‚îÇ Migration donn√©es                  (3j) ‚úÖ    ‚îÇ
‚îÇ Sprint 3    ‚îÇ Refactoring Sync Engine            (3j) ‚úÖ    ‚îÇ
‚îÇ Sprint 4    ‚îÇ Refactoring DuckDBRepository            (2j)  ‚îÇ
‚îÇ Sprint 5    ‚îÇ Refactoring UI Big Bang                 (3j)  ‚îÇ
‚îÇ Sprint 6    ‚îÇ Optimisation API                        (2j)  ‚îÇ
‚îÇ Sprint 7    ‚îÇ Tests & Couverture                      (2j)  ‚îÇ
‚îÇ Sprint 8    ‚îÇ Finalisation & Release v5.0             (2j)  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ TOTAL       ‚îÇ 18 jours ouvr√©s (peut descendre √† 14j)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Parall√©lisation Possible

- **Sprints 3 & 4** : Peuvent se chevaucher (Sync Engine ‚â† Repository)
- **Gain** : -4 jours ‚Üí **14 jours total**

---

## 10. Commandes Rapides (Cheat Sheet)

### Avant de Commencer

```bash
# Backup complet
python scripts/backup_all_players.py --output backups/pre-v5-$(date +%Y%m%d)

# Baseline tests
python -m pytest -q --ignore=tests/integration

# Audit donn√©es
python scripts/audit_current_data.py --summary
```

### Pendant la Migration

```bash
# Cr√©er shared_matches.duckdb
python scripts/migration/create_shared_matches_db.py

# Migrer un joueur
python scripts/migration/migrate_player_to_shared.py GAMERTAG --verbose

# Valider migration
python scripts/migration/validate_migration.py GAMERTAG

# Tests
python -m pytest tests/migration/ -v
```

### Validation Finale

```bash
# Suite compl√®te
python -m pytest --cov=src --cov-report=html

# Benchmark
python scripts/benchmark_v4_vs_v5.py --detailed

# Check qualit√©
black src/ tests/ scripts/
ruff check src/
mypy src/ --ignore-missing-imports
```

---

## 11. Contact & Support

**Questions** : Cr√©er une issue dans le repo  
**Bugs** : Tag `bug` + `v5-migration`  
**Documentation** : `.ai/` et `docs/`

---

**Fin du Plan v5.0** üöÄ
