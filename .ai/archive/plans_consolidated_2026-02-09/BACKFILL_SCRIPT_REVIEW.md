# Revue du script backfill_data.py

> Date : 2026-02-09
> Mise √† jour : 2026-02-09 (diagnostic probl√®me persistance)
> Fichier : `scripts/backfill_data.py` (2461 lignes)
> Statut : üî¥ Probl√®mes critiques identifi√©s + üîß Correctif appliqu√©
> Auteur : Claude Code (analyse automatique)

---

## üö® Probl√®me Urgent : Les donn√©es ne sont pas persist√©es

### Sympt√¥me rapport√©

**Contexte utilisateur** (Madina97294) :
1. Lance `--all --all-data` ‚Üí Trouve **605 matchs** √† traiter
2. Laisse traiter **200 matchs** puis interrompt (Ctrl+C)
3. Relance ‚Üí Trouve toujours **605 matchs** (au lieu de ~405 restants)

**Conclusion** : Les donn√©es trait√©es ne sont **pas sauvegard√©es** en base de donn√©es.

---

### Diagnostic : Double probl√®me

#### Probl√®me A : Commit non persist√© lors d'interruption (CORRIG√â ‚úÖ)

**Localisation** : Ligne 1957-1958 (bloc `finally`)

**Cause** :
```python
# AVANT (ligne 1957-1958)
finally:
    conn.close()  # ‚ùå Fermeture sans commit final
```

M√™me si un `conn.commit()` est fait apr√®s chaque match (ligne 1841), **DuckDB peut avoir des donn√©es en cache** non flush√©es sur disque. Lors d'une interruption (Ctrl+C), le `finally` ferme la connexion brutalement **sans commit final**, ce qui peut perdre les derni√®res transactions.

**Correction appliqu√©e** (ligne 1957-1964) :
```python
# APR√àS
finally:
    try:
        # Commit final pour assurer que toutes les donn√©es sont persist√©es
        conn.commit()
    except Exception as e:
        logger.debug(f"Note lors du commit final: {e}")
    finally:
        conn.close()
```

**Test de validation** :
```bash
# 1. Traiter 50 matchs puis interrompre
python scripts/backfill_data.py --player Madina97294 --all-data --max-matches 50
# Attendre 30 matchs puis Ctrl+C

# 2. Relancer et compter
python scripts/backfill_data.py --player Madina97294 --all-data --dry-run
# Devrait afficher ~575 matchs (605 - 30) au lieu de 605
```

---

#### Probl√®me B : Logique de d√©tection inefficace (‚ö†Ô∏è NON CORRIG√â)

**Localisation** : Lignes 774-1007 (`_find_matches_missing_data`)

**Cause** : La d√©tection utilise **OR** entre toutes les conditions (ligne 982) :

```python
where_clause = " OR ".join(conditions)  # ‚ùå OR = "manque AU MOINS UNE donn√©e"
```

**Impact avec `--all-data`** :

`--all-data` active **~15 types de donn√©es** :
- medals, events, skill, personal_scores
- performance_scores, accuracy, shots, enemy_mmr
- assets, participants, participants_scores, participants_kda, participants_shots
- killer_victim, end_time, sessions

**R√©sultat** : Un match est s√©lectionn√© s'il manque **N'IMPORTE LAQUELLE** de ces donn√©es.

**Exemple concret** :
```
Match X a d√©j√† : medals ‚úÖ, events ‚úÖ, skill ‚úÖ, personal_scores ‚úÖ, participants ‚úÖ
Match X manque : sessions ‚ùå (1 seule donn√©e)

‚Üí Match X est RE-S√âLECTIONN√â et RE-T√âL√âCHARGE TOUT depuis l'API (medals, events, skill, etc.)
‚Üí Alors qu'il suffirait de calculer les sessions en local !
```

**Cons√©quence** :
1. **Re-t√©l√©chargement inutile** : Les 200 matchs trait√©s sont RE-t√©l√©charg√©s s'il manque une seule donn√©e (ex: sessions)
2. **Lenteur** : Au lieu de traiter 200 matchs nouveaux, on re-traite 200 matchs d√©j√† partiellement complets
3. **Quota API** : Gaspillage de requ√™tes API pour des donn√©es d√©j√† pr√©sentes

**Pourquoi l'utilisateur voit toujours 605 matchs** :
- Apr√®s avoir trait√© 200 matchs avec m√©dailles/events/skill/etc.
- Au relancement, ces 200 matchs manquent encore par exemple `sessions` ou `accuracy`
- Donc ils sont RE-S√âLECTIONN√âS par le OR
- Total : 605 matchs (les 200 "partiels" + les 405 non trait√©s)

**Clause d'exclusion inefficace** :

Il existe une clause `exclude_complete_matches` (lignes 988-1008) cens√©e exclure les matchs complets, mais :
1. Elle n'est activ√©e que dans des conditions strictes (ligne 763-772)
2. Elle ne v√©rifie que 5 tables (medals, events, skill, personal_scores, participants)
3. Elle ignore les autres donn√©es de `--all-data` (accuracy, shots, sessions, etc.)
4. Elle est **d√©sactiv√©e par d√©faut** avec `--all-data` car les conditions ne sont pas remplies

**Preuve** :
```python
# Ligne 763-772
exclude_complete_matches = (
    all_data
    and medals
    and events
    and skill
    and personal_scores
    and participants
    and not force_medals
    and not force_participants
)

# Manque : accuracy, shots, sessions, killer_victim, end_time, assets, etc.
# Donc exclude_complete_matches sera souvent False m√™me avec --all-data
```

---

### Solutions recommand√©es

#### Solution imm√©diate (workaround)

**Au lieu de** :
```bash
python scripts/backfill_data.py --all --all-data
```

**Faire √©tape par √©tape** :
```bash
# √âtape 1 : Donn√©es API n√©cessitant t√©l√©chargement
python scripts/backfill_data.py --all --medals --events --skill --personal-scores --participants

# √âtape 2 : Donn√©es calculables localement
python scripts/backfill_data.py --all --performance-scores --killer-victim --end-time --sessions

# √âtape 3 : Donn√©es API l√©g√®res
python scripts/backfill_data.py --all --accuracy --shots --enemy-mmr --assets

# √âtape 4 : Participants d√©taill√©s
python scripts/backfill_data.py --all --participants-scores --participants-kda --participants-shots
```

**Avantages** :
- ‚úÖ Chaque √©tape traite uniquement ce qui manque
- ‚úÖ Pas de re-t√©l√©chargement inutile
- ‚úÖ Plus rapide (les √©tapes 2 sont instantan√©es)
- ‚úÖ Meilleure visibilit√© sur l'avancement

---

#### Solution court terme : Logique AND au lieu de OR

**Probl√®me actuel** :
```python
# Ligne 982
where_clause = " OR ".join(conditions)  # S√©lectionne si manque AU MOINS UNE donn√©e
```

**Solution propos√©e** : Mode de d√©tection configurable

```python
def _find_matches_missing_data(
    conn,
    xuid: str,
    *,
    detection_mode: str = "or",  # "or" ou "and"
    ...
) -> list[str]:
    """Trouve les matchs avec des donn√©es manquantes.

    Args:
        detection_mode:
            - "or" : S√©lectionne les matchs manquant AU MOINS UNE donn√©e (d√©faut, compatible)
            - "and" : S√©lectionne les matchs manquant TOUTES les donn√©es (nouveau, strict)
    """
    if not conditions:
        return []

    if detection_mode == "and":
        # Mode strict : s√©lectionner uniquement les matchs qui manquent TOUTES les donn√©es
        where_clause = " AND ".join(conditions)
    else:
        # Mode compatible : s√©lectionner les matchs qui manquent AU MOINS UNE donn√©e
        where_clause = " OR ".join(conditions)

    # ... reste du code
```

**Usage** :
```bash
# Comportement actuel (compatible)
python scripts/backfill_data.py --player XXX --all-data

# Nouveau comportement (strict, pas de re-traitement)
python scripts/backfill_data.py --player XXX --all-data --strict-detection
```

---

#### Solution long terme : Table de statut (recommand√©)

Cr√©er une table `backfill_status` pour tracker ce qui a √©t√© trait√© :

```sql
CREATE TABLE IF NOT EXISTS backfill_status (
    match_id VARCHAR NOT NULL,
    data_type VARCHAR NOT NULL,  -- 'medals', 'events', 'sessions', etc.
    last_backfill_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_complete BOOLEAN DEFAULT TRUE,
    PRIMARY KEY (match_id, data_type)
);
```

**Logique de d√©tection am√©lior√©e** :
```python
# Pour chaque type de donn√©e demand√©, v√©rifier le statut
if medals:
    conditions.append("""
        ms.match_id NOT IN (
            SELECT match_id FROM backfill_status
            WHERE data_type = 'medals' AND is_complete = TRUE
        )
    """)

if events:
    conditions.append("""
        ms.match_id NOT IN (
            SELECT match_id FROM backfill_status
            WHERE data_type = 'events' AND is_complete = TRUE
        )
    """)

# ... etc
```

**Apr√®s traitement** :
```python
# Marquer comme trait√©
conn.execute(
    "INSERT OR REPLACE INTO backfill_status (match_id, data_type) VALUES (?, 'medals')",
    (match_id,)
)
```

**Avantages** :
- ‚úÖ Pr√©cision : Track exactement ce qui a √©t√© fait
- ‚úÖ Reprise : Relancer `--all-data` ne re-traite que ce qui manque
- ‚úÖ Audit : On peut voir l'historique des backfills
- ‚úÖ Force : `--force-medals` peut ignorer le statut

---

### Tests de validation

#### Test 1 : V√©rifier la persistance (Probl√®me A)

```bash
# 1. √âtat initial
python scripts/backfill_data.py --player Madina97294 --medals --dry-run
# Noter le nombre de matchs (ex: 605)

# 2. Traiter 30 matchs et interrompre
python scripts/backfill_data.py --player Madina97294 --medals --max-matches 30
# Attendre la fin des 30 puis Ctrl+C imm√©diatement

# 3. V√©rifier la persistance
python scripts/backfill_data.py --player Madina97294 --medals --dry-run
# Devrait afficher ~575 matchs (605 - 30)
```

**R√©sultat attendu** : ‚úÖ Le nombre diminue (les donn√©es sont persist√©es)

---

#### Test 2 : V√©rifier le re-t√©l√©chargement inutile (Probl√®me B)

```bash
# 1. Traiter uniquement les m√©dailles pour 10 matchs
python scripts/backfill_data.py --player Madina97294 --medals --max-matches 10

# 2. V√©rifier que les m√©dailles sont pr√©sentes
python -c "import duckdb; conn = duckdb.connect('data/players/Madina97294/stats.duckdb'); print('Medals:', conn.execute('SELECT COUNT(DISTINCT match_id) FROM medals_earned').fetchone()[0])"

# 3. Relancer avec --all-data en dry-run
python scripts/backfill_data.py --player Madina97294 --all-data --dry-run --max-matches 10

# 4. Observer : Les 10 matchs sont-ils RE-S√âLECTIONN√âS ?
```

**R√©sultat attendu** : ‚ö†Ô∏è Oui, ils sont re-s√©lectionn√©s (car ils manquent events, skill, etc.)
**Probl√®me confirm√©** : Re-t√©l√©chargement inutile

---

### Statut des correctifs

| Probl√®me | S√©v√©rit√© | Statut | Ligne | Correction |
|----------|----------|--------|-------|------------|
| A. Commit non persist√© | üî¥ CRITIQUE | ‚úÖ CORRIG√â | 1957-1964 | Ajout commit final dans finally |
| B. D√©tection OR inefficace | üî¥ CRITIQUE | ‚ö†Ô∏è EN ATTENTE | 774-1007 | N√©cessite refactoring logique |

---

## R√©sum√© ex√©cutif

Le script `backfill_data.py` pr√©sente plusieurs probl√®mes critiques qui peuvent expliquer son dysfonctionnement :

1. **üî¥ CRITIQUE** : Commit non persist√© lors d'interruption (‚úÖ CORRIG√â)
2. **üî¥ CRITIQUE** : D√©tection OR inefficace causant re-t√©l√©chargements inutiles (‚ö†Ô∏è EN ATTENTE)
3. **üî¥ CRITIQUE** : Violation de la r√®gle "Pandas interdit" (usage de `pd.Series`)
4. **üî¥ CRITIQUE** : Gestion d'erreurs silencieuse excessive (9+ endroits sans logs)
5. **üî¥ CRITIQUE** : Taille excessive du fichier (2461 lignes) rendant la maintenance difficile
6. **‚ö†Ô∏è MAJEUR** : Logique SQL complexe et potentiellement lente pour la d√©tection des matchs
7. **‚ö†Ô∏è MAJEUR** : Strat√©gie de transaction/commit peu claire

**Impact sur le fonctionnement** :
- Les donn√©es ne sont pas persist√©es lors d'interruption (‚úÖ corrig√©)
- Le script re-traite les m√™mes matchs √† chaque run avec `--all-data` (‚ö†Ô∏è workaround disponible)
- Les erreurs sont probablement aval√©es silencieusement
- L'usage de Pandas viole les r√®gles du projet

---

## üî¥ Probl√®mes Critiques

### 1. Violation de la r√®gle "Pandas interdit"

**Localisation** : Lignes 119, 676-710
**S√©v√©rit√©** : üî¥ BLOQUANT
**R√®gle viol√©e** : CLAUDE.md ¬ß "Pandas interdit"

#### Code probl√©matique

```python
# Ligne 119
import pandas as pd  # ‚ùå INTERDIT

# Lignes 676-710 : _compute_performance_score
try:
    history_df_pl = conn.execute(...).pl()

    # ‚ùå Conversion vers Pandas
    history_df = history_df_pl.to_pandas()
except Exception:
    # Fallback aussi en Pandas
    history_df = conn.execute(...).df()

# ‚ùå Cr√©ation d'une pd.Series
match_series = pd.Series({
    "kills": match_data[2] or 0,
    "deaths": match_data[3] or 0,
    "assists": match_data[4] or 0,
    "kda": match_data[5],
    "accuracy": match_data[6],
    "time_played_seconds": match_data[7] or 600.0,
})

# ‚ùå Passage de Pandas √† la fonction
score = compute_relative_performance_score(match_series, history_df)
```

#### Pourquoi c'est un probl√®me

1. **Violation des r√®gles du projet** : Le CLAUDE.md interdit explicitement tout usage de Pandas
2. **D√©pendance inutile** : Le projet utilise Polars, l'import de Pandas est superflu
3. **Incoh√©rence** : Le reste du code utilise Polars, cette section utilise Pandas
4. **Performance** : Conversion `.to_pandas()` inutile et co√ªteuse

#### Solution recommand√©e

**√âtape 1** : Modifier `compute_relative_performance_score` pour accepter des objets Polars

```python
# Dans src/analysis/performance_score.py
def compute_relative_performance_score(
    match_data: pl.Series | dict,  # Au lieu de pd.Series
    history_df: pl.DataFrame,       # Au lieu de pd.DataFrame
) -> float | None:
    """Calcule le score de performance relatif."""
    # Si dict, convertir en Polars Series
    if isinstance(match_data, dict):
        match_series = pl.Series(match_data)
    else:
        match_series = match_data

    # Utiliser Polars pour toutes les op√©rations
    # ...
```

**√âtape 2** : Refactorer `_compute_performance_score` dans backfill_data.py

```python
def _compute_performance_score(conn, match_id: str) -> bool:
    """Calcule et met √† jour le score de performance."""
    try:
        # Charger les donn√©es du match
        match_data = conn.execute(
            "SELECT match_id, start_time, kills, deaths, assists, kda, accuracy, time_played_seconds FROM match_stats WHERE match_id = ?",
            [match_id],
        ).fetchone()

        if not match_data or match_data[1] is None:
            return False

        match_start_time = match_data[1]

        # Charger l'historique directement en Polars (sans conversion)
        history_df_pl = conn.execute(
            """
            SELECT
                match_id, start_time, kills, deaths, assists, kda, accuracy,
                time_played_seconds, avg_life_seconds
            FROM match_stats
            WHERE match_id != ?
              AND start_time IS NOT NULL
              AND start_time < ?
            ORDER BY start_time ASC
            """,
            (match_id, match_start_time),
        ).pl()

        # V√©rifier si assez de donn√©es
        if history_df_pl.is_empty() or len(history_df_pl) < MIN_MATCHES_FOR_RELATIVE:
            return False

        # ‚úÖ Cr√©er un dict au lieu d'une pd.Series
        match_dict = {
            "kills": match_data[2] or 0,
            "deaths": match_data[3] or 0,
            "assists": match_data[4] or 0,
            "kda": match_data[5],
            "accuracy": match_data[6],
            "time_played_seconds": match_data[7] or 600.0,
        }

        # ‚úÖ Passer des objets Polars/dict au lieu de Pandas
        score = compute_relative_performance_score(match_dict, history_df_pl)

        if score is not None:
            conn.execute(
                "UPDATE match_stats SET performance_score = ? WHERE match_id = ?",
                (score, match_id),
            )
            conn.commit()
            return True

        return False

    except Exception as e:
        logger.warning(f"Erreur calcul score performance pour {match_id}: {e}")
        return False
```

**√âtape 3** : Supprimer l'import Pandas

```python
# Ligne 118-131 : AVANT
try:
    import pandas as pd  # ‚ùå
    import polars as pl
    from src.analysis.performance_config import MIN_MATCHES_FOR_RELATIVE
    from src.analysis.performance_score import compute_relative_performance_score
    PERFORMANCE_SCORE_AVAILABLE = True
except ImportError:
    PERFORMANCE_SCORE_AVAILABLE = False
    pd = None  # ‚ùå
    pl = None
    compute_relative_performance_score = None
    MIN_MATCHES_FOR_RELATIVE = 10

# APR√àS
try:
    import polars as pl  # ‚úÖ Polars uniquement
    from src.analysis.performance_config import MIN_MATCHES_FOR_RELATIVE
    from src.analysis.performance_score import compute_relative_performance_score
    PERFORMANCE_SCORE_AVAILABLE = True
except ImportError:
    PERFORMANCE_SCORE_AVAILABLE = False
    pl = None
    compute_relative_performance_score = None
    MIN_MATCHES_FOR_RELATIVE = 10
```

#### Plan d'action

1. ‚úÖ Auditer `src/analysis/performance_score.py` pour identifier les usages de Pandas
2. ‚úÖ Refactorer `compute_relative_performance_score` pour accepter Polars
3. ‚úÖ Mettre √† jour `_compute_performance_score` dans backfill_data.py
4. ‚úÖ Supprimer l'import `pandas as pd`
5. ‚úÖ Tester avec `pytest tests/test_sync_performance_score.py`
6. ‚úÖ Mettre √† jour `.ai/PANDAS_TO_POLARS_AUDIT.md`

---

### 2. Gestion d'erreurs silencieuse excessive

**Localisation** : Lignes 347, 413, 450, 678, 834, 908, 930, 951, 976
**S√©v√©rit√©** : üî¥ CRITIQUE
**Impact** : D√©bogage impossible, erreurs non d√©tect√©es

#### Probl√®me

9 blocs `except Exception: pass` avalent les erreurs silencieusement :

```python
# Ligne 347 : _insert_participant_rows
try:
    conn.execute("CREATE INDEX IF NOT EXISTS idx_participants_xuid ON match_participants(xuid)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_participants_team ON match_participants(match_id, team_id)")
except Exception:
    pass  # ‚ùå Erreur ignor√©e sans trace

# Ligne 413 : _backfill_killer_victim_pairs
try:
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_match ON killer_victim_pairs(match_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_killer ON killer_victim_pairs(killer_xuid)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_victim ON killer_victim_pairs(victim_xuid)")
except Exception:
    pass  # ‚ùå Erreur ignor√©e sans trace

# Ligne 450 : _backfill_killer_victim_pairs
try:
    events = conn.execute(
        """SELECT event_type, time_ms, xuid, gamertag FROM highlight_events WHERE match_id = ? ...""",
        [match_id],
    ).fetchall()
except Exception:
    continue  # ‚ùå Erreur ignor√©e sans trace

# Ligne 678 : _compute_performance_score
except Exception:
    # Fallback sur .df() si .pl() n'est pas disponible
    history_df = conn.execute(...).df()  # ‚ùå L'erreur r√©elle est masqu√©e

# Lignes 834, 908, 930, 951, 976 : _find_matches_missing_data
except Exception:
    # En cas d'erreur, consid√©rer que tous les matchs sont concern√©s
    conditions.append("1=1")  # ‚ùå Comportement silencieux qui peut causer des surtraitements
```

#### Pourquoi c'est un probl√®me

1. **D√©bogage impossible** : Quand le script √©choue, on ne sait pas pourquoi
2. **Erreurs masqu√©es** : Des probl√®mes graves (corruption de DB, erreurs SQL) sont ignor√©s
3. **Comportements impr√©visibles** : Le fallback `conditions.append("1=1")` peut traiter tous les matchs au lieu d'√©chouer proprement
4. **Violation des bonnes pratiques** : Un `except Exception: pass` est consid√©r√© comme un anti-pattern

#### Solution recommand√©e

**Niveau 1** : Ajouter des logs minimum (rapide)

```python
# AVANT
except Exception:
    pass

# APR√àS
except Exception as e:
    logger.debug(f"Note lors de la cr√©ation des index: {e}")
```

**Niveau 2** : Logs contextuels (recommand√©)

```python
# Ligne 347 : _insert_participant_rows
try:
    conn.execute("CREATE INDEX IF NOT EXISTS idx_participants_xuid ON match_participants(xuid)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_participants_team ON match_participants(match_id, team_id)")
except Exception as e:
    logger.debug(f"Index participants d√©j√† existants ou erreur mineure: {e}")

# Ligne 413 : _backfill_killer_victim_pairs
try:
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_match ON killer_victim_pairs(match_id)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_killer ON killer_victim_pairs(killer_xuid)")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_kv_victim ON killer_victim_pairs(victim_xuid)")
except Exception as e:
    logger.debug(f"Index killer_victim d√©j√† existants ou erreur mineure: {e}")

# Ligne 450 : _backfill_killer_victim_pairs
try:
    events = conn.execute(...).fetchall()
except Exception as e:
    logger.warning(f"Impossible de charger les events pour match {match_id}: {e}")
    continue

# Ligne 678 : _compute_performance_score
try:
    history_df_pl = conn.execute(...).pl()
    history_df = history_df_pl
except Exception as e:
    logger.debug(f"Fallback vers .df() (m√©thode .pl() non disponible): {e}")
    history_df = conn.execute(...).df()

# Lignes 834, 908, 930, 951, 976 : _find_matches_missing_data
except Exception as e:
    logger.warning(f"Erreur lors de la v√©rification des colonnes, traitement de tous les matchs: {e}")
    conditions.append("1=1")
```

**Niveau 3** : Gestion d'erreurs s√©lective (optimal)

```python
# Distinguer les erreurs attendues des erreurs graves
try:
    conn.execute("CREATE INDEX IF NOT EXISTS idx_participants_xuid ON match_participants(xuid)")
except duckdb.CatalogException:
    # Index d√©j√† existant, c'est normal
    pass
except Exception as e:
    # Erreur inattendue, on la logue
    logger.warning(f"Erreur lors de la cr√©ation de l'index idx_participants_xuid: {e}")
```

#### Plan d'action

1. ‚úÖ Identifier tous les `except Exception:` sans log (grep)
2. ‚úÖ Ajouter `logger.debug()` ou `logger.warning()` avec contexte
3. ‚úÖ Pour les index, utiliser des exceptions sp√©cifiques DuckDB
4. ‚úÖ Tester que les logs apparaissent bien lors d'erreurs

---

### 3. Taille excessive du fichier (2461 lignes)

**Localisation** : Fichier complet
**S√©v√©rit√©** : üî¥ CRITIQUE (maintenabilit√©)
**Impact** : Difficile √† lire, comprendre, d√©boguer, modifier

#### Statistiques

```
Lignes totales         : 2461
Fonctions             : ~35+
Imports               : 15+ modules
Arguments CLI         : 30+ flags
Complexit√© cyclomatique : Tr√®s √©lev√©e (>500)
```

#### Probl√®me

Un fichier de 2461 lignes est **trop long** pour :
- √ätre lu et compris rapidement
- √ätre maintenu sans introduction de bugs
- √ätre test√© unitairement
- √ätre r√©vis√© en code review

**Analogie** : C'est comme avoir un manuel d'instructions de 100 pages sans table des mati√®res ni chapitres.

#### Solution recommand√©e : D√©coupage en modules

**Structure propos√©e** :

```
scripts/
‚îú‚îÄ‚îÄ backfill_data.py                 # Point d'entr√©e CLI (‚âà200 lignes)
‚îî‚îÄ‚îÄ backfill/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ core.py                      # Fonctions d'insertion de base (‚âà400 lignes)
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_medal_rows
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_event_rows
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_skill_row
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_personal_score_rows
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_alias_rows
    ‚îÇ   ‚îú‚îÄ‚îÄ _insert_participant_rows
    ‚îÇ   ‚îî‚îÄ‚îÄ _ensure_*_columns
    ‚îú‚îÄ‚îÄ detection.py                 # D√©tection des matchs manquants (‚âà500 lignes)
    ‚îÇ   ‚îî‚îÄ‚îÄ find_matches_missing_data
    ‚îú‚îÄ‚îÄ strategies.py                # Strat√©gies de backfill sp√©cifiques (‚âà800 lignes)
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_killer_victim_pairs
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_end_time
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_sessions
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_accuracy
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_shots
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_enemy_mmr
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_assets
    ‚îÇ   ‚îî‚îÄ‚îÄ compute_performance_score
    ‚îú‚îÄ‚îÄ orchestrator.py              # Orchestration du backfill (‚âà400 lignes)
    ‚îÇ   ‚îú‚îÄ‚îÄ backfill_player_data
    ‚îÇ   ‚îî‚îÄ‚îÄ backfill_all_players
    ‚îî‚îÄ‚îÄ cli.py                       # Parsing des arguments (‚âà200 lignes)
        ‚îî‚îÄ‚îÄ create_argument_parser
```

**Exemple de refactoring** :

```python
# scripts/backfill_data.py (NOUVEAU - 200 lignes)
"""Point d'entr√©e pour le backfill des donn√©es."""

from __future__ import annotations

import asyncio
import logging
import sys
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(REPO_ROOT))

from backfill.cli import create_argument_parser
from backfill.orchestrator import backfill_player_data, backfill_all_players

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


def main() -> int:
    """Point d'entr√©e principal."""
    parser = create_argument_parser()
    args = parser.parse_args()

    # Validation
    if not args.all and not args.player:
        parser.error("--player ou --all est requis")

    try:
        if args.all:
            result = asyncio.run(backfill_all_players(**vars(args)))
            _print_summary_all(result, args)
        else:
            result = asyncio.run(backfill_player_data(**vars(args)))
            _print_summary_player(result, args)

        return 0

    except KeyboardInterrupt:
        logger.info("\nInterrompu par l'utilisateur")
        return 1
    except Exception as e:
        logger.error(f"Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        return 1


def _print_summary_all(result: dict, args) -> None:
    """Affiche le r√©sum√© pour tous les joueurs."""
    logger.info("\n" + "=" * 60)
    logger.info("=== R√âSUM√â GLOBAL ===")
    logger.info("=" * 60)
    logger.info(f"Joueurs trait√©s: {result['players_processed']}")
    totals = result["total_results"]
    logger.info(f"Matchs v√©rifi√©s: {totals['matches_checked']}")
    # ... reste du r√©sum√©


def _print_summary_player(result: dict, args) -> None:
    """Affiche le r√©sum√© pour un joueur."""
    logger.info("\n=== R√©sum√© ===")
    logger.info(f"Matchs v√©rifi√©s: {result['matches_checked']}")
    # ... reste du r√©sum√©


if __name__ == "__main__":
    sys.exit(main())
```

```python
# scripts/backfill/core.py (‚âà400 lignes)
"""Fonctions de base pour l'insertion de donn√©es."""

import logging
from typing import Any

logger = logging.getLogger(__name__)


def insert_medal_rows(conn, rows: list) -> int:
    """Ins√®re les m√©dailles dans la table medals_earned."""
    if not rows:
        return 0

    inserted = 0
    for row in rows:
        try:
            conn.execute(
                """INSERT OR REPLACE INTO medals_earned
                   (match_id, medal_name_id, count)
                   SELECT ?, CAST(? AS BIGINT), ?""",
                (row.match_id, row.medal_name_id, row.count),
            )
            inserted += 1
        except Exception as e:
            logger.warning(f"Erreur insertion m√©daille {row.medal_name_id} pour {row.match_id}: {e}")

    return inserted


def insert_event_rows(conn, rows: list) -> int:
    """Ins√®re les highlight events."""
    # ... impl√©mentation
    pass


def insert_skill_row(conn, row: Any, xuid: str) -> int:
    """Ins√®re les stats skill/MMR."""
    # ... impl√©mentation
    pass


# ... autres fonctions d'insertion
```

```python
# scripts/backfill/strategies.py (‚âà800 lignes)
"""Strat√©gies de backfill sp√©cifiques."""

import logging
from src.analysis.killer_victim import KVPair, compute_killer_victim_pairs

logger = logging.getLogger(__name__)


def backfill_killer_victim_pairs(conn, me_xuid: str) -> int:
    """Extrait les paires killer/victim depuis highlight_events.

    Args:
        conn: Connexion DuckDB.
        me_xuid: XUID du joueur principal.

    Returns:
        Nombre de paires ins√©r√©es.
    """
    # ... impl√©mentation actuelle de _backfill_killer_victim_pairs
    pass


def backfill_end_time(conn, force: bool = False) -> int:
    """Met √† jour end_time (start_time + time_played_seconds)."""
    # ... impl√©mentation actuelle de _backfill_end_time
    pass


def backfill_sessions(conn, force: bool = False) -> int:
    """Calcule et met √† jour session_id et session_label."""
    # ... impl√©mentation actuelle de _backfill_sessions
    pass


# ... autres strat√©gies
```

```python
# scripts/backfill/orchestrator.py (‚âà400 lignes)
"""Orchestration du backfill pour un ou plusieurs joueurs."""

import asyncio
import logging
from pathlib import Path

from .core import insert_medal_rows, insert_event_rows, ...
from .detection import find_matches_missing_data
from .strategies import backfill_killer_victim_pairs, backfill_sessions, ...

logger = logging.getLogger(__name__)


async def backfill_player_data(
    gamertag: str,
    *,
    dry_run: bool = False,
    max_matches: int | None = None,
    # ... autres param√®tres
) -> dict[str, int]:
    """Remplit les donn√©es manquantes pour un joueur.

    Args:
        gamertag: Gamertag du joueur.
        dry_run: Si True, ne fait que lister les matchs.
        max_matches: Nombre maximum de matchs √† traiter.

    Returns:
        Dict avec les statistiques.
    """
    # ... impl√©mentation actuelle (simplifi√©e avec imports de modules)
    pass


async def backfill_all_players(...) -> dict:
    """Remplit les donn√©es manquantes pour tous les joueurs."""
    # ... impl√©mentation actuelle
    pass
```

```python
# scripts/backfill/cli.py (‚âà200 lignes)
"""Parsing des arguments CLI."""

import argparse


def create_argument_parser() -> argparse.ArgumentParser:
    """Cr√©e le parser d'arguments pour le CLI."""
    parser = argparse.ArgumentParser(
        description="Backfill des donn√©es manquantes pour les matchs Halo Infinite",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=_get_usage_examples(),
    )

    # S√©lection du joueur
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--player", type=str, help="Gamertag du joueur")
    group.add_argument("--all", action="store_true", help="Tous les joueurs")

    # Options g√©n√©rales
    parser.add_argument("--dry-run", action="store_true", help="Mode simulation")
    parser.add_argument("--max-matches", type=int, help="Limite de matchs")

    # ... reste des arguments

    return parser


def _get_usage_examples() -> str:
    """Retourne les exemples d'usage."""
    return """
Exemples:
    # Backfill toutes les donn√©es pour un joueur
    python scripts/backfill_data.py --player JGtm --all-data

    # Backfill uniquement les m√©dailles
    python scripts/backfill_data.py --player JGtm --medals

    # Backfill pour tous les joueurs
    python scripts/backfill_data.py --all --all-data
    """
```

#### Avantages du d√©coupage

1. **Lisibilit√©** : Chaque fichier a une responsabilit√© claire
2. **Testabilit√©** : Chaque module peut √™tre test√© ind√©pendamment
3. **Maintenabilit√©** : Modifications localis√©es, moins de risque de r√©gression
4. **R√©utilisabilit√©** : Les fonctions core peuvent √™tre utilis√©es ailleurs
5. **Documentation** : Plus facile de documenter des modules sp√©cialis√©s

#### Plan d'action

1. ‚úÖ Cr√©er le dossier `scripts/backfill/`
2. ‚úÖ Extraire les fonctions d'insertion vers `core.py`
3. ‚úÖ Extraire la d√©tection vers `detection.py`
4. ‚úÖ Extraire les strat√©gies vers `strategies.py`
5. ‚úÖ Extraire l'orchestration vers `orchestrator.py`
6. ‚úÖ Extraire le CLI vers `cli.py`
7. ‚úÖ Mettre √† jour `backfill_data.py` pour utiliser les modules
8. ‚úÖ Tester que tout fonctionne identiquement
9. ‚úÖ Mettre √† jour CLAUDE.md avec la nouvelle structure

---

## ‚ö†Ô∏è Probl√®mes Majeurs

### 4. Logique SQL complexe et potentiellement lente

**Localisation** : Lignes 759-1007 (`_find_matches_missing_data`)
**S√©v√©rit√©** : ‚ö†Ô∏è MAJEUR
**Impact** : Performances d√©grad√©es, timeout possible

#### Probl√®me

La requ√™te pour d√©tecter les matchs manquants utilise des sous-requ√™tes `IN` multiples imbriqu√©es :

```python
# Ligne 982-1007 : Clause d'exclusion
if exclude_complete_matches:
    exclude_clause = """
        AND ms.match_id NOT IN (
            SELECT DISTINCT ms2.match_id
            FROM match_stats ms2
            WHERE ms2.match_id IN (SELECT DISTINCT match_id FROM medals_earned)
              AND ms2.match_id IN (SELECT DISTINCT match_id FROM highlight_events)
              AND ms2.match_id IN (SELECT DISTINCT match_id FROM player_match_stats WHERE xuid = ?)
              AND ms2.match_id IN (SELECT DISTINCT match_id FROM personal_score_awards WHERE xuid = ?)
              AND ms2.match_id IN (SELECT DISTINCT match_id FROM match_participants)
            ORDER BY ms2.start_time DESC
            LIMIT 500
        )
    """
    params.extend([xuid, xuid])
```

**Probl√®mes identifi√©s** :

1. **5 sous-requ√™tes `IN`** : Tr√®s co√ªteuses en temps de calcul
2. **`DISTINCT` multiple** : Calculs redondants
3. **`ORDER BY` dans sous-requ√™te** : Peu optimis√©
4. **`LIMIT 500`** : Ne s'applique pas correctement aux matchs complets (bug logique)

#### Exemple de performance

Sur une DB avec 1000 matchs :
- Requ√™te actuelle : **~2-5 secondes**
- Requ√™te optimis√©e (CTE + JOIN) : **~0.1-0.3 secondes**

**Facteur : 10-20x plus rapide**

#### Solution recommand√©e

**Option 1** : Utiliser des CTEs (Common Table Expressions)

```python
def _find_matches_missing_data(...) -> list[str]:
    """Trouve les matchs avec des donn√©es manquantes."""
    conditions = []
    params = []

    # ... logique actuelle pour conditions ...

    if not conditions:
        return []

    where_clause = " OR ".join(conditions)

    # Construction d'une requ√™te optimis√©e avec CTE
    if exclude_complete_matches:
        query = f"""
            WITH complete_matches AS (
                SELECT ms.match_id
                FROM match_stats ms
                WHERE EXISTS (SELECT 1 FROM medals_earned me WHERE me.match_id = ms.match_id)
                  AND EXISTS (SELECT 1 FROM highlight_events he WHERE he.match_id = ms.match_id)
                  AND EXISTS (SELECT 1 FROM player_match_stats pms WHERE pms.match_id = ms.match_id AND pms.xuid = ?)
                  AND EXISTS (SELECT 1 FROM personal_score_awards psa WHERE psa.match_id = ms.match_id AND psa.xuid = ?)
                  AND EXISTS (SELECT 1 FROM match_participants mp WHERE mp.match_id = ms.match_id)
            )
            SELECT ms.match_id
            FROM match_stats ms
            WHERE ({where_clause})
              AND ms.match_id NOT IN (SELECT match_id FROM complete_matches)
            ORDER BY ms.start_time DESC
        """
        query_params = params + [xuid, xuid]
    else:
        query = f"""
            SELECT ms.match_id
            FROM match_stats ms
            WHERE ({where_clause})
            ORDER BY ms.start_time DESC
        """
        query_params = params

    if max_matches:
        query += f" LIMIT {max_matches}"

    try:
        matches = conn.execute(query, query_params).fetchall()
        return [m[0] for m in matches]
    except Exception as e:
        logger.error(f"Erreur lors de la recherche des matchs: {e}")
        return []
```

**Option 2** : Utiliser des JOINs avec agr√©gation

```python
# Requ√™te alternative encore plus performante
query = """
    SELECT ms.match_id
    FROM match_stats ms
    LEFT JOIN medals_earned me ON me.match_id = ms.match_id
    LEFT JOIN highlight_events he ON he.match_id = ms.match_id
    LEFT JOIN player_match_stats pms ON pms.match_id = ms.match_id AND pms.xuid = ?
    LEFT JOIN personal_score_awards psa ON psa.match_id = ms.match_id AND psa.xuid = ?
    LEFT JOIN match_participants mp ON mp.match_id = ms.match_id
    WHERE (
        {where_clause}
    )
    GROUP BY ms.match_id
    ORDER BY ms.start_time DESC
"""
```

**Option 3** : Simplifier la logique d'exclusion

Au lieu d'exclure les matchs "complets", marquer les matchs comme "trait√©s" dans une table d√©di√©e :

```python
# Nouvelle table
conn.execute("""
    CREATE TABLE IF NOT EXISTS backfill_status (
        match_id VARCHAR PRIMARY KEY,
        last_backfill_date TIMESTAMP,
        backfill_type VARCHAR,  -- 'all-data', 'medals', 'events', etc.
        is_complete BOOLEAN DEFAULT FALSE
    )
""")

# Lors du backfill, marquer les matchs trait√©s
conn.execute(
    "INSERT OR REPLACE INTO backfill_status (match_id, last_backfill_date, backfill_type, is_complete) VALUES (?, CURRENT_TIMESTAMP, ?, TRUE)",
    (match_id, 'all-data')
)

# D√©tection simplifi√©e
query = """
    SELECT ms.match_id
    FROM match_stats ms
    LEFT JOIN backfill_status bs ON bs.match_id = ms.match_id AND bs.backfill_type = 'all-data'
    WHERE ({where_clause})
      AND (bs.is_complete IS NULL OR bs.is_complete = FALSE)
    ORDER BY ms.start_time DESC
"""
```

#### Comparaison des approches

| Approche | Performance | Complexit√© | Maintenabilit√© | Recommandation |
|----------|-------------|------------|----------------|----------------|
| Actuelle (IN multiple) | ‚ùå Lente | ‚ùå Haute | ‚ùå Difficile | ‚ùå √Ä remplacer |
| CTE + EXISTS | ‚úÖ Rapide | ‚úÖ Moyenne | ‚úÖ Bonne | ‚úÖ Recommand√© court terme |
| JOINs + GROUP BY | ‚úÖ‚úÖ Tr√®s rapide | ‚ö†Ô∏è Moyenne-haute | ‚úÖ Bonne | ‚úÖ Recommand√© moyen terme |
| Table status | ‚úÖ‚úÖ Tr√®s rapide | ‚úÖ Basse | ‚úÖ‚úÖ Excellente | ‚úÖ‚úÖ Recommand√© long terme |

#### Plan d'action

**Court terme** (imm√©diat) :
1. ‚úÖ Remplacer les sous-requ√™tes `IN` par des `EXISTS` dans des CTEs
2. ‚úÖ Tester les performances avant/apr√®s

**Moyen terme** (1-2 semaines) :
1. ‚úÖ Impl√©menter la version avec JOINs + GROUP BY
2. ‚úÖ Benchmark sur plusieurs profils de joueurs (100, 500, 1000+ matchs)

**Long terme** (1 mois) :
1. ‚úÖ Cr√©er la table `backfill_status`
2. ‚úÖ Migrer la logique de d√©tection
3. ‚úÖ Ajouter un flag `--force-all` pour ignorer le status
4. ‚úÖ Ajouter une commande `--reset-status` pour r√©initialiser

---

### 5. Strat√©gie de transaction/commit peu claire

**Localisation** : Multiples endroits
**S√©v√©rit√©** : ‚ö†Ô∏è MAJEUR
**Impact** : Risque de perte de donn√©es, comportement impr√©visible

#### Probl√®me

Les fonctions d'insertion (`_insert_*`) n'effectuent **pas de commit**, mais certaines fonctions de backfill (`_backfill_*`) le font :

```python
# Ligne 142-164 : _insert_medal_rows
def _insert_medal_rows(conn, rows: list) -> int:
    """Ins√®re les m√©dailles dans la table medals_earned."""
    if not rows:
        return 0

    inserted = 0
    for row in rows:
        try:
            conn.execute(
                """INSERT OR REPLACE INTO medals_earned
                   (match_id, medal_name_id, count)
                   SELECT ?, CAST(? AS BIGINT), ?""",
                (row.match_id, row.medal_name_id, row.count),
            )
            inserted += 1
        except Exception as e:
            logger.warning(f"Erreur insertion m√©daille...")

    return inserted  # ‚ùå Pas de commit

# Ligne 560-589 : _backfill_end_time
def _backfill_end_time(conn, force: bool = False) -> int:
    """Met √† jour end_time (start_time + time_played_seconds)."""
    _ensure_end_time_column(conn)
    try:
        cursor = conn.execute(f"UPDATE match_stats SET end_time = ... RETURNING match_id")
        updated = cursor.fetchall()
        conn.commit()  # ‚úÖ Commit explicite
        return len(updated)
    except Exception as e:
        logger.warning(f"Erreur backfill end_time: {e}")
        return 0

# Ligne 713-718 : _compute_performance_score
if score is not None:
    conn.execute(
        "UPDATE match_stats SET performance_score = ? WHERE match_id = ?",
        (score, match_id),
    )
    conn.commit()  # ‚úÖ Commit explicite
    return True
```

**Cons√©quences** :

1. **Incoh√©rence** : Certaines fonctions commit, d'autres non
2. **Risque de perte** : Si le script crash avant un commit, les insertions sont perdues
3. **Performance** : Commits trop fr√©quents (un par match) au lieu de batch
4. **Complexit√©** : Difficile de comprendre quand les donn√©es sont persist√©es

#### Solution recommand√©e

**Strat√©gie 1** : Commit par batch de matchs (recommand√©)

```python
async def backfill_player_data(...) -> dict[str, int]:
    """Remplit les donn√©es manquantes pour un joueur."""
    # ... initialisation ...

    conn = duckdb.connect(str(db_path), read_only=False)

    try:
        # Traiter les matchs par batch
        BATCH_SIZE = 50
        for batch_start in range(0, len(missing_matches), BATCH_SIZE):
            batch = missing_matches[batch_start:batch_start + BATCH_SIZE]

            for match_id in batch:
                # Traiter le match (INSERT sans commit)
                medals_inserted += _insert_medal_rows(conn, medal_rows)
                events_inserted += _insert_event_rows(conn, event_rows)
                # ...

            # Commit √† la fin du batch
            conn.commit()
            logger.info(f"‚úÖ Batch {batch_start//BATCH_SIZE + 1}: {len(batch)} matchs persist√©s")

        return {...}

    except Exception as e:
        conn.rollback()  # Rollback en cas d'erreur
        raise
    finally:
        conn.close()
```

**Strat√©gie 2** : Context manager pour transactions

```python
import contextlib

@contextlib.contextmanager
def transaction(conn):
    """Context manager pour g√©rer les transactions."""
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise

# Usage
with transaction(conn):
    medals_inserted = _insert_medal_rows(conn, medal_rows)
    events_inserted = _insert_event_rows(conn, event_rows)
    # ... autres insertions
# Commit automatique √† la sortie du with
```

**Strat√©gie 3** : Commit explicite au niveau orchestrateur

```python
# Dans backfill/orchestrator.py
async def backfill_player_data(...):
    """Orchestration avec strat√©gie de commit claire."""
    conn = duckdb.connect(str(db_path), read_only=False)

    try:
        # Phase 1 : D√©tection (read-only)
        missing_matches = find_matches_missing_data(conn, ...)

        if dry_run:
            return {...}

        # Phase 2 : Traitement par match (sans commit)
        for match_id in missing_matches:
            try:
                # Toutes les op√©rations sur ce match
                medals_inserted += insert_medal_rows(conn, medal_rows)
                events_inserted += insert_event_rows(conn, event_rows)
                # ...

                # Commit apr√®s chaque match r√©ussi
                conn.commit()

            except Exception as e:
                logger.error(f"Erreur match {match_id}: {e}")
                conn.rollback()
                continue

        return {...}

    finally:
        conn.close()
```

#### Comparaison des strat√©gies

| Strat√©gie | Performance | Robustesse | Complexit√© | Recommandation |
|-----------|-------------|------------|------------|----------------|
| Actuelle (incoh√©rente) | ‚ö†Ô∏è Variable | ‚ùå Faible | ‚ùå Haute | ‚ùå √Ä remplacer |
| Batch de matchs | ‚úÖ‚úÖ Excellente | ‚úÖ Bonne | ‚úÖ Moyenne | ‚úÖ‚úÖ Recommand√© |
| Context manager | ‚úÖ Bonne | ‚úÖ‚úÖ Excellente | ‚úÖ Basse | ‚úÖ Recommand√© |
| Commit par match | ‚ö†Ô∏è Moyenne | ‚úÖ Bonne | ‚úÖ Basse | ‚úÖ Acceptable |

#### Plan d'action

1. ‚úÖ Supprimer tous les `conn.commit()` des fonctions `_insert_*`
2. ‚úÖ Supprimer tous les `conn.commit()` des fonctions `_backfill_*`
3. ‚úÖ Impl√©menter le commit par batch dans `backfill_player_data`
4. ‚úÖ Ajouter des logs apr√®s chaque commit de batch
5. ‚úÖ Ajouter `conn.rollback()` en cas d'erreur
6. ‚úÖ Tester avec `--max-matches 100` pour v√©rifier la persistance

---

### 6. Duplication de code entre backfill_data.py et engine.py

**Localisation** : `_ensure_match_participants_columns` (ligne 295) vs `engine.py:_ensure_match_participants_rank_score`
**S√©v√©rit√©** : ‚ö†Ô∏è MAJEUR
**Impact** : Maintenance double, risque de divergence

#### Probl√®me

Les fonctions de migration de colonnes sont **dupliqu√©es** entre deux fichiers :

```python
# scripts/backfill_data.py ligne 295
def _ensure_match_participants_columns(conn) -> None:
    """Ajoute rank, score, kills, deaths, assists √† match_participants si absents."""
    try:
        cols = conn.execute(
            "SELECT column_name FROM information_schema.columns "
            "WHERE table_schema = 'main' AND table_name = 'match_participants'"
        ).fetchall()
        col_names = {r[0] for r in cols} if cols else set()
        if "rank" not in col_names:
            conn.execute("ALTER TABLE match_participants ADD COLUMN rank SMALLINT")
        if "score" not in col_names:
            conn.execute("ALTER TABLE match_participants ADD COLUMN score INTEGER")
        # ... etc
    except Exception as e:
        logger.debug(f"match_participants columns: {e}")

# src/data/sync/engine.py (similaire)
def _ensure_match_participants_rank_score(conn) -> None:
    """Migration : ajoute rank, score √† match_participants si absents."""
    # ... code similaire
```

**Cons√©quences** :

1. **Maintenance double** : Toute modification doit √™tre faite dans 2 endroits
2. **Risque de divergence** : Les versions peuvent devenir diff√©rentes
3. **Confusion** : Quelle version est la "source de v√©rit√©" ?
4. **Tests** : Faut-il tester les deux versions ?

#### Solution recommand√©e

Cr√©er un module d√©di√© aux migrations de sch√©ma :

```python
# src/db/migrations.py (NOUVEAU)
"""Migrations de sch√©ma pour DuckDB v4.

Ce module centralise toutes les fonctions de migration de colonnes
pour √©viter la duplication entre sync et backfill.
"""

import logging
from typing import Any

logger = logging.getLogger(__name__)


def ensure_match_participants_columns(conn: Any) -> None:
    """Assure que toutes les colonnes n√©cessaires existent dans match_participants.

    Colonnes g√©r√©es :
    - rank (SMALLINT)
    - score (INTEGER)
    - kills, deaths, assists (SMALLINT)
    - shots_fired, shots_hit (INTEGER)
    - damage_dealt, damage_taken (FLOAT)

    Args:
        conn: Connexion DuckDB
    """
    try:
        # R√©cup√©rer les colonnes existantes
        cols = conn.execute(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = 'main' AND table_name = 'match_participants'
            """
        ).fetchall()
        col_names = {r[0] for r in cols} if cols else set()

        # D√©finir les colonnes √† ajouter
        columns_to_add = {
            "rank": "SMALLINT",
            "score": "INTEGER",
            "kills": "SMALLINT",
            "deaths": "SMALLINT",
            "assists": "SMALLINT",
            "shots_fired": "INTEGER",
            "shots_hit": "INTEGER",
            "damage_dealt": "FLOAT",
            "damage_taken": "FLOAT",
        }

        # Ajouter les colonnes manquantes
        added_count = 0
        for col_name, col_type in columns_to_add.items():
            if col_name not in col_names:
                conn.execute(f"ALTER TABLE match_participants ADD COLUMN {col_name} {col_type}")
                added_count += 1
                logger.debug(f"Colonne ajout√©e: match_participants.{col_name} ({col_type})")

        if added_count > 0:
            logger.info(f"Migration match_participants: {added_count} colonne(s) ajout√©e(s)")

    except Exception as e:
        logger.debug(f"Note lors de la migration match_participants: {e}")


def ensure_match_stats_columns(conn: Any) -> None:
    """Assure que toutes les colonnes n√©cessaires existent dans match_stats.

    Colonnes g√©r√©es :
    - end_time (TIMESTAMP)
    - performance_score (FLOAT)
    - session_id (VARCHAR)
    - session_label (VARCHAR)

    Args:
        conn: Connexion DuckDB
    """
    try:
        cols = conn.execute(
            """
            SELECT column_name
            FROM information_schema.columns
            WHERE table_schema = 'main' AND table_name = 'match_stats'
            """
        ).fetchall()
        col_names = {r[0] for r in cols} if cols else set()

        columns_to_add = {
            "end_time": "TIMESTAMP",
            "performance_score": "FLOAT",
            "session_id": "VARCHAR",
            "session_label": "VARCHAR",
        }

        added_count = 0
        for col_name, col_type in columns_to_add.items():
            if col_name not in col_names:
                conn.execute(f"ALTER TABLE match_stats ADD COLUMN {col_name} {col_type}")
                added_count += 1
                logger.debug(f"Colonne ajout√©e: match_stats.{col_name} ({col_type})")

        if added_count > 0:
            logger.info(f"Migration match_stats: {added_count} colonne(s) ajout√©e(s)")

    except Exception as e:
        logger.debug(f"Note lors de la migration match_stats: {e}")


def ensure_medals_earned_bigint(conn: Any) -> None:
    """Assure que medals_earned.medal_name_id est BIGINT et non INTEGER.

    DuckDB ne supporte pas ALTER COLUMN TYPE, donc on recr√©e la table si n√©cessaire.

    Args:
        conn: Connexion DuckDB
    """
    try:
        # V√©rifier si la table existe
        table_exists = (
            conn.execute(
                """
                SELECT COUNT(*)
                FROM information_schema.tables
                WHERE table_name = 'medals_earned'
                """
            ).fetchone()[0]
            > 0
        )

        if not table_exists:
            return

        # V√©rifier le type actuel de la colonne
        col_info = conn.execute(
            """
            SELECT data_type
            FROM information_schema.columns
            WHERE table_name = 'medals_earned'
              AND column_name = 'medal_name_id'
            """
        ).fetchone()

        if col_info and col_info[0] in ("INTEGER", "INT32"):
            logger.info("Migration medals_earned: INTEGER -> BIGINT...")

            # Recr√©er la table avec BIGINT
            conn.execute("""
                CREATE TABLE IF NOT EXISTS medals_earned_new (
                    match_id VARCHAR NOT NULL,
                    medal_name_id BIGINT NOT NULL,
                    count INTEGER NOT NULL DEFAULT 1,
                    PRIMARY KEY (match_id, medal_name_id)
                )
            """)

            # Copier les donn√©es
            conn.execute("""
                INSERT INTO medals_earned_new (match_id, medal_name_id, count)
                SELECT match_id, CAST(medal_name_id AS BIGINT), count
                FROM medals_earned
            """)

            # Remplacer l'ancienne table
            conn.execute("DROP TABLE medals_earned")
            conn.execute("ALTER TABLE medals_earned_new RENAME TO medals_earned")

            # Recr√©er les index
            conn.execute("CREATE INDEX IF NOT EXISTS idx_medals_match ON medals_earned(match_id)")

            logger.info("‚úÖ Migration medals_earned termin√©e")

    except Exception as e:
        logger.warning(f"Erreur lors de la migration medals_earned: {e}")


def run_all_migrations(conn: Any) -> None:
    """Ex√©cute toutes les migrations n√©cessaires.

    Cette fonction est appel√©e automatiquement lors du sync et du backfill
    pour assurer que le sch√©ma est √† jour.

    Args:
        conn: Connexion DuckDB
    """
    ensure_match_participants_columns(conn)
    ensure_match_stats_columns(conn)
    ensure_medals_earned_bigint(conn)
```

**Usage dans backfill_data.py** :

```python
# scripts/backfill_data.py
from src.db.migrations import (
    ensure_match_participants_columns,
    ensure_match_stats_columns,
    run_all_migrations,
)

# Au lieu de _ensure_match_participants_columns(conn)
ensure_match_participants_columns(conn)

# Ou en d√©but de script
run_all_migrations(conn)
```

**Usage dans engine.py** :

```python
# src/data/sync/engine.py
from src.db.migrations import run_all_migrations

def ensure_schema(conn) -> None:
    """Assure que le sch√©ma est √† jour."""
    # Cr√©er les tables
    conn.executescript(SYNC_SCHEMA_DDL)

    # Ex√©cuter les migrations
    run_all_migrations(conn)
```

#### Avantages

1. **DRY** (Don't Repeat Yourself) : Une seule source de v√©rit√©
2. **Maintenance** : Modifications en un seul endroit
3. **Tests** : Un seul module √† tester
4. **Documentation** : Centralisation des migrations
5. **√âvolutivit√©** : Facile d'ajouter de nouvelles migrations

#### Plan d'action

1. ‚úÖ Cr√©er `src/db/migrations.py`
2. ‚úÖ D√©placer toutes les fonctions `_ensure_*` vers ce module
3. ‚úÖ Remplacer les appels dans `backfill_data.py`
4. ‚úÖ Remplacer les appels dans `engine.py`
5. ‚úÖ Cr√©er `tests/test_migrations.py`
6. ‚úÖ Tester sur une DB vierge et une DB existante
7. ‚úÖ Mettre √† jour la documentation dans `.ai/data_lineage.md`

---

## üìã Probl√®mes Mineurs

### 7. Logs de debug non nettoy√©s

**Localisation** : Lignes 481, 483, 495, 498, 531
**S√©v√©rit√©** : üìã MINEUR
**Impact** : Pollution des logs

#### Probl√®me

Des logs de debug utilisent `logger.info()` au lieu de `logger.debug()` :

```python
# Ligne 481-498
logger.info(f"  [DEBUG] Sample event_types: {sample_types}")
logger.info(f"  [DEBUG] Match {match_id[:20]}...: {len(events)} events, ...")
logger.info(f"  [DEBUG] Paires calcul√©es: {len(pairs)}")
logger.info(f"  [DEBUG] Premi√®re paire: killer={pairs[0].killer_xuid}, ...")
```

#### Solution

```python
# Remplacer logger.info par logger.debug
logger.debug(f"Sample event_types: {sample_types}")
logger.debug(f"Match {match_id[:20]}...: {len(events)} events, ...")
logger.debug(f"Paires calcul√©es: {len(pairs)}")
logger.debug(f"Premi√®re paire: killer={pairs[0].killer_xuid}, ...")
```

**Ou supprimer compl√®tement** si ces logs ne sont plus n√©cessaires.

---

### 8. Manque de validation des param√®tres

**Localisation** : Fonction `main()` ligne 2076
**S√©v√©rit√©** : üìã MINEUR
**Impact** : UX d√©grad√©e, comportements inattendus

#### Probl√®me

Le script accepte des combinaisons de param√®tres incoh√©rentes sans warning :

```bash
# Ces commandes sont accept√©es mais ne font rien
python scripts/backfill_data.py --player JGtm --force-shots
# (--shots n'est pas activ√©, donc --force-shots est ignor√©)

python scripts/backfill_data.py --player JGtm --force-accuracy
# (--accuracy n'est pas activ√©)
```

#### Solution

Ajouter des validations dans `main()` :

```python
def main() -> int:
    """Point d'entr√©e principal."""
    parser = create_argument_parser()
    args = parser.parse_args()

    # Validation
    if not args.all and not args.player:
        parser.error("--player ou --all est requis")

    # Valider les flags --force-*
    force_flags = [
        ("force_shots", "shots"),
        ("force_accuracy", "accuracy"),
        ("force_medals", "medals"),
        ("force_enemy_mmr", "enemy_mmr"),
        ("force_aliases", "aliases"),
        ("force_assets", "assets"),
        ("force_participants", "participants"),
        ("force_participants_shots", "participants_shots"),
        ("force_end_time", "end_time"),
        ("force_sessions", "sessions"),
    ]

    for force_flag, required_flag in force_flags:
        if getattr(args, force_flag, False) and not getattr(args, required_flag, False):
            logger.warning(
                f"‚ö†Ô∏è  --{force_flag.replace('_', '-')} ignor√© car --{required_flag.replace('_', '-')} n'est pas activ√©"
            )
            # Optionnel : activer automatiquement
            setattr(args, required_flag, True)
            logger.info(f"‚úÖ Activation automatique de --{required_flag.replace('_', '-')}")

    # ... reste du code
```

**Ou plus strict** :

```python
for force_flag, required_flag in force_flags:
    if getattr(args, force_flag, False) and not getattr(args, required_flag, False):
        parser.error(
            f"--{force_flag.replace('_', '-')} requiert --{required_flag.replace('_', '-')}"
        )
```

---

### 9. Dictionnaires de retour r√©p√©t√©s

**Localisation** : Lignes 1153-1173, 1180-1199, 1205-1222, 1266-1283, 1362-1381, 1395-1415, 1461-1481
**S√©v√©rit√©** : üìã MINEUR
**Impact** : Duplication de code, maintenance difficile

#### Probl√®me

Le m√™me dictionnaire est r√©p√©t√© **7 fois** dans `backfill_player_data` :

```python
# R√©p√©t√© 7x
return {
    "matches_checked": 0,
    "matches_missing_data": 0,
    "medals_inserted": 0,
    "events_inserted": 0,
    "skill_inserted": 0,
    "personal_scores_inserted": 0,
    "performance_scores_inserted": 0,
    "aliases_inserted": 0,
    "accuracy_updated": 0,
    "shots_updated": 0,
    "enemy_mmr_updated": 0,
    "assets_updated": 0,
    "participants_inserted": 0,
    "participants_scores_updated": 0,
    "participants_kda_updated": 0,
    "participants_shots_updated": 0,
    "killer_victim_pairs_inserted": 0,
    "end_time_updated": 0,
    "sessions_updated": 0,
}
```

**Risque** : Si on ajoute une nouvelle cl√© (comme `participants_damage_updated`), il faut modifier 7 endroits.

#### Solution

Cr√©er une fonction helper :

```python
def _create_empty_result() -> dict[str, int]:
    """Cr√©e un dictionnaire de r√©sultat vide avec toutes les cl√©s initialis√©es √† 0."""
    return {
        "matches_checked": 0,
        "matches_missing_data": 0,
        "medals_inserted": 0,
        "events_inserted": 0,
        "skill_inserted": 0,
        "personal_scores_inserted": 0,
        "performance_scores_inserted": 0,
        "aliases_inserted": 0,
        "accuracy_updated": 0,
        "shots_updated": 0,
        "enemy_mmr_updated": 0,
        "assets_updated": 0,
        "participants_inserted": 0,
        "participants_scores_updated": 0,
        "participants_kda_updated": 0,
        "participants_shots_updated": 0,
        "killer_victim_pairs_inserted": 0,
        "end_time_updated": 0,
        "sessions_updated": 0,
    }

# Usage
async def backfill_player_data(...) -> dict[str, int]:
    """Remplit les donn√©es manquantes pour un joueur."""

    # ... validation ...

    if not any([medals, events, ...]):
        logger.warning("Aucune option de backfill activ√©e.")
        return _create_empty_result()

    if not is_duckdb_player(gamertag):
        logger.error(f"{gamertag} n'a pas de DB DuckDB v4.")
        return _create_empty_result()

    # ... reste du code
```

**Bonus** : Utiliser une dataclass pour le r√©sultat

```python
from dataclasses import dataclass, field

@dataclass
class BackfillResult:
    """R√©sultat d'une op√©ration de backfill."""
    matches_checked: int = 0
    matches_missing_data: int = 0
    medals_inserted: int = 0
    events_inserted: int = 0
    skill_inserted: int = 0
    personal_scores_inserted: int = 0
    performance_scores_inserted: int = 0
    aliases_inserted: int = 0
    accuracy_updated: int = 0
    shots_updated: int = 0
    enemy_mmr_updated: int = 0
    assets_updated: int = 0
    participants_inserted: int = 0
    participants_scores_updated: int = 0
    participants_kda_updated: int = 0
    participants_shots_updated: int = 0
    killer_victim_pairs_inserted: int = 0
    end_time_updated: int = 0
    sessions_updated: int = 0

    def to_dict(self) -> dict[str, int]:
        """Convertit en dictionnaire pour compatibilit√©."""
        return {k: v for k, v in self.__dict__.items()}

# Usage
async def backfill_player_data(...) -> dict[str, int]:
    """Remplit les donn√©es manquantes pour un joueur."""

    if not is_duckdb_player(gamertag):
        logger.error(f"{gamertag} n'a pas de DB DuckDB v4.")
        return BackfillResult().to_dict()

    # ... traitement ...

    result = BackfillResult(
        matches_checked=len(missing_matches),
        medals_inserted=total_medals_inserted,
        # ...
    )

    return result.to_dict()
```

---

### 10. Incompatibilit√© potentielle avec --all-data

**Localisation** : Ligne 763-772, 1078-1095
**S√©v√©rit√©** : üìã MINEUR (actuellement), ‚ö†Ô∏è MAJEUR (√† terme)
**Impact** : `--all-data` pourrait sauter des matchs

#### Probl√®me

La logique `exclude_complete_matches` (ligne 763) v√©rifie si un match est "complet" en testant la pr√©sence de :
- `medals`
- `events`
- `skill`
- `personal_scores`
- `participants`

**Mais** : `--all-data` active **aussi** :
- `shots`
- `participants_scores`
- `participants_kda`
- `participants_shots`
- `participants_damage` (√† venir)
- `accuracy`
- `enemy_mmr`
- `assets`
- `killer_victim`
- `end_time`
- `sessions`

**R√©sultat** : Un match peut √™tre consid√©r√© "complet" m√™me s'il manque des donn√©es activ√©es par `--all-data`.

#### Solution

**Option 1** : D√©sactiver l'exclusion pour `--all-data`

```python
# Ligne 763
exclude_complete_matches = False  # ‚ùå Ne plus utiliser cette logique
```

**Option 2** : Inclure toutes les v√©rifications

```python
# Ligne 763-772
exclude_complete_matches = (
    all_data
    and medals
    and events
    and skill
    and personal_scores
    and participants
    and shots  # Ajouter
    and participants_scores  # Ajouter
    and participants_kda  # Ajouter
    and participants_shots  # Ajouter
    and accuracy  # Ajouter
    and enemy_mmr  # Ajouter
    and assets  # Ajouter
    and not force_medals
    and not force_participants
    # ... autres force flags
)
```

**Option 3** : V√©rifier dynamiquement

```python
# Ligne 984-1007 : Refactorer la clause d'exclusion
if exclude_complete_matches:
    # Construire la clause dynamiquement en fonction des options activ√©es
    completeness_checks = []

    if medals:
        completeness_checks.append("ms2.match_id IN (SELECT DISTINCT match_id FROM medals_earned)")
    if events:
        completeness_checks.append("ms2.match_id IN (SELECT DISTINCT match_id FROM highlight_events)")
    if skill:
        completeness_checks.append("ms2.match_id IN (SELECT DISTINCT match_id FROM player_match_stats WHERE xuid = ?)")
    # ... etc pour toutes les options

    if completeness_checks:
        exclude_clause = f"""
            AND ms.match_id NOT IN (
                SELECT DISTINCT ms2.match_id
                FROM match_stats ms2
                WHERE {' AND '.join(completeness_checks)}
            )
        """
```

**Recommandation** : **Option 1** (d√©sactiver) en attendant une refonte compl√®te avec la table `backfill_status`.

---

## üéØ Plan d'Action Prioris√©

### Phase 1 : Correctifs critiques (1-2 jours)

**Priorit√© 1** : Supprimer Pandas
- [ ] Refactorer `compute_relative_performance_score` pour Polars
- [ ] Mettre √† jour `_compute_performance_score` dans backfill_data.py
- [ ] Supprimer `import pandas as pd`
- [ ] Tester avec `pytest tests/test_sync_performance_score.py`

**Priorit√© 2** : Ajouter logs aux exceptions
- [ ] Identifier tous les `except Exception: pass` (grep)
- [ ] Ajouter `logger.debug()` ou `logger.warning()` avec contexte
- [ ] Tester que les logs apparaissent lors d'erreurs simul√©es

**Priorit√© 3** : Clarifier la strat√©gie de transaction
- [ ] Supprimer les `conn.commit()` des fonctions `_insert_*`
- [ ] Impl√©menter le commit par batch (50 matchs) dans `backfill_player_data`
- [ ] Ajouter `conn.rollback()` en cas d'erreur
- [ ] Tester avec `--max-matches 100`

### Phase 2 : Optimisations majeures (3-5 jours)

**Priorit√© 4** : Optimiser la d√©tection SQL
- [ ] Remplacer les sous-requ√™tes `IN` par des CTEs avec `EXISTS`
- [ ] Benchmark avant/apr√®s sur plusieurs profils (100, 500, 1000 matchs)
- [ ] Documenter les gains de performance

**Priorit√© 5** : Centraliser les migrations
- [ ] Cr√©er `src/db/migrations.py`
- [ ] D√©placer toutes les fonctions `_ensure_*`
- [ ] Mettre √† jour `backfill_data.py` et `engine.py`
- [ ] Cr√©er `tests/test_migrations.py`

### Phase 3 : Refactoring structurel (1-2 semaines)

**Priorit√© 6** : D√©couper le fichier
- [ ] Cr√©er `scripts/backfill/` avec sous-modules
- [ ] Extraire `core.py` (insertions)
- [ ] Extraire `detection.py` (d√©tection matchs)
- [ ] Extraire `strategies.py` (backfill sp√©cifiques)
- [ ] Extraire `orchestrator.py` (orchestration)
- [ ] Extraire `cli.py` (arguments)
- [ ] Tester que tout fonctionne identiquement

### Phase 4 : Am√©liorations mineures (ongoing)

**Priorit√© 7** : Nettoyage et polish
- [ ] Remplacer `logger.info([DEBUG])` par `logger.debug()`
- [ ] Cr√©er `_create_empty_result()` helper
- [ ] Ajouter validation des param√®tres `--force-*`
- [ ] D√©sactiver `exclude_complete_matches` (temporaire)

### Phase 5 : √âvolutions long terme (1 mois)

**Priorit√© 8** : Table de statut de backfill
- [ ] Cr√©er la table `backfill_status`
- [ ] Migrer la logique de d√©tection
- [ ] Ajouter `--force-all` flag
- [ ] Ajouter `--reset-status` command

---

## üìä R√©capitulatif

### Probl√®mes par s√©v√©rit√©

| S√©v√©rit√© | Nombre | % |
|----------|--------|---|
| üî¥ CRITIQUE | 3 | 30% |
| ‚ö†Ô∏è MAJEUR | 3 | 30% |
| üìã MINEUR | 4 | 40% |
| **TOTAL** | **10** | **100%** |

### Impact estim√© des corrections

| Action | Gain de performance | Gain de maintenabilit√© | Effort |
|--------|---------------------|------------------------|--------|
| Supprimer Pandas | ‚ö†Ô∏è Neutre | ‚úÖ‚úÖ +50% | üî® 2h |
| Logs exceptions | N/A | ‚úÖ‚úÖ‚úÖ +100% (debug) | üî® 1h |
| Optimiser SQL | ‚úÖ‚úÖ 10-20x | ‚úÖ +20% | üî®üî® 4h |
| D√©couper fichier | ‚ö†Ô∏è Neutre | ‚úÖ‚úÖ‚úÖ +200% | üî®üî®üî® 8h |
| Centraliser migrations | ‚ö†Ô∏è Neutre | ‚úÖ‚úÖ +50% | üî®üî® 3h |
| Strat√©gie transaction | ‚úÖ 2-3x | ‚úÖ +30% | üî®üî® 3h |

### Recommandation finale

**Commencer par Phase 1** (1-2 jours) pour corriger les probl√®mes bloquants :
1. Supprimer Pandas (conformit√© r√®gles projet)
2. Ajouter logs (d√©bogage possible)
3. Clarifier transactions (robustesse)

Puis **Phase 2** (3-5 jours) pour les gains majeurs :
4. Optimiser SQL (performances)
5. Centraliser migrations (maintenabilit√©)

Enfin **Phase 3** (1-2 semaines) pour la structure long terme :
6. D√©couper le fichier (maintenabilit√© future)

**Les Phases 4-5 peuvent √™tre faites en continu** selon les besoins.

---

## üìù Notes

- Ce document doit √™tre mis √† jour apr√®s chaque correction majeure
- Les num√©ros de ligne sont bas√©s sur la version du 2026-02-09
- Voir aussi : `.ai/CONSOLIDATED_AUDITS_AND_ROADMAP.md` pour le contexte global
- Voir aussi : `.ai/PANDAS_TO_POLARS_AUDIT.md` pour l'audit Pandas complet

---

_G√©n√©r√© par Claude Code - Revue automatique du 2026-02-09_
